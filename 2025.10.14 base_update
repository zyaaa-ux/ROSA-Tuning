import os
import math
import json
import time
import atexit
from dataclasses import dataclass
from typing import Dict, List, Optional, Tuple

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch import Tensor

import numpy as np

from concurrent.futures import ProcessPoolExecutor, as_completed
import multiprocessing as mp

from datasets import load_from_disk
from transformers import (
    AutoConfig,
    AutoTokenizer,
    Trainer,
    TrainingArguments,
    set_seed,
)
from transformers.models.qwen3.modeling_qwen3 import (
    Qwen3ForCausalLM,
    Qwen3DecoderLayer,
)

import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor
import threading
import os
import atexit

MODEL_LOCAL_DIR = ""
DATASET_DIR = ""
OUTPUT_DIR = ""
os.makedirs(OUTPUT_DIR, exist_ok=True)

USE_FLASH_ATTN = True
BF16 = True if torch.cuda.is_available() else False

SEQ_LEN = 0
ATTN_WINDOW = 0
FIRST_GLOBAL_LAYERS = 0

ROSA_NUM_ROUTES = 0
ROSA_VOCAB_SIZE = 0
LCG_TOPK = 0
LCG_POS_SUBSAMPLE = 0
ROSA_CPU_WORKERS = 0
ROSA_POOL_CHUNKSIZE = 0

LR_ROSA = 0
LR_BACKBONE = 0
WEIGHT_DECAY = 0
WARMUP_STEPS = 0
NUM_EPOCHS = 0
PER_DEVICE_TRAIN_BSZ = 0
GRAD_ACCUM_STEPS = 0
LOGGING_STEPS = 0
EVAL_STEPS = 0
SEED = 0

SAVE_STATE_DICT_NAME = ""

GRADIENT_CHECKPOINTING = bool(LR_BACKBONE and LR_BACKBONE > 0.0)

_ROSA_POOL = None
_ROSA_POOL_PID = None
_ROSA_POOL_LOCK = threading.Lock()

def _env_int(k, default):
    try:
        return int(os.environ.get(k, default))
    except Exception:
        return default

LOCAL_WORLD_SIZE = _env_int("LOCAL_WORLD_SIZE", max(1, torch.cuda.device_count()))
LOCAL_RANK = _env_int("LOCAL_RANK", 0)
GLOBAL_RANK = _env_int("RANK", 0)

def _available_logical_cpus() -> int:
    try:
        if hasattr(os, "sched_getaffinity"):
            return max(1, len(os.sched_getaffinity(0)))
    except Exception:
        pass

    def _parse_cpuset(s: str) -> int:
        n = 0
        for part in s.split(","):
            part = part.strip()
            if not part:
                continue
            if "-" in part:
                a, b = part.split("-")
                n += int(b) - int(a) + 1
            else:
                n += 1
        return n

    try:
        for p in ("/sys/fs/cgroup/cpuset.cpus", "/sys/fs/cgroup/cpuset/cpuset.cpus"):
            if os.path.exists(p):
                with open(p) as f:
                    txt = f.read().strip()
                if txt:
                    return max(1, _parse_cpuset(txt))
    except Exception:
        pass

    return max(1, os.cpu_count() or 1)

_TOTAL_CPUS = _available_logical_cpus()
_PER_RANK_CPUS = max(1, _TOTAL_CPUS // max(1, LOCAL_WORLD_SIZE))

torch.set_num_threads(_PER_RANK_CPUS)

def _rosa_worker_init():
    os.environ.setdefault("OMP_NUM_THREADS", "1")
    os.environ.setdefault("MKL_NUM_THREADS", "1")
    try:
        import torch as _t
        _t.set_num_threads(1)
    except Exception:
        pass

import numpy as _np
from typing import List, Tuple

class _SAMExactCPU:
    __slots__ = ("next", "link", "length", "e", "last", "size", "K")

    def __init__(self, max_states: int, K: int):
        self.K = int(K)
        S = int(max_states)
        self.next   = _np.full((S, self.K), -1, dtype=_np.int32)
        self.link   = _np.full((S,),       -1, dtype=_np.int32)
        self.length = _np.zeros((S,),          dtype=_np.int32)
        self.e      = _np.full((S,),       -1, dtype=_np.int32)
        self.last   = 0
        self.size   = 1

    def _new_state(self, L: int) -> int:
        s = self.size
        self.size += 1
        self.length[s] = L
        self.link[s] = -1
        self.e[s] = -1
        self.next[s, :].fill(-1)
        return s

    def extend(self, x: int, pos: int):
        cur = self._new_state(self.length[self.last] + 1)
        p = self.last
        while p != -1 and self.next[p, x] == -1:
            self.next[p, x] = cur
            p = self.link[p]
        if p == -1:
            self.link[cur] = 0
        else:
            q = int(self.next[p, x])
            if self.length[p] + 1 == self.length[q]:
                self.link[cur] = q
            else:
                clone = self._new_state(self.length[p] + 1)
                self.next[clone, : ] = self.next[q, : ]
                self.link[clone]     = self.link[q]
                self.e[clone]        = self.e[q]
                while p != -1 and self.next[p, x] == q:
                    self.next[p, x] = clone
                    p = self.link[p]
                self.link[q]   = clone
                self.link[cur] = clone
        v = cur
        while v != -1 and self.e[v] != pos:
            self.e[v] = pos
            v = self.link[v]
        self.last = cur

    def match_next(self, x: int) -> int:
        p = self.last
        while p != -1 and self.next[p, x] == -1:
            p = self.link[p]
        return -1 if p == -1 else int(self.next[p, x])

def _count_runs(z: List[int]) -> int:
    if not z: return 0
    cnt, last = 1, z[0]
    for x in z[1:]:
        if x != last:
            cnt += 1
            last = x
    return cnt

@dataclass
class FixedLenLMCollator:
    pad_token_id: int
    seq_len: int

    def __call__(self, features):
        input_ids = [f["input_ids"][: self.seq_len] for f in features]
        if "attention_mask" in features[0]:
            attention_mask = [f["attention_mask"][: self.seq_len] for f in features]
        else:
            attention_mask = [[1] * len(x) for x in input_ids]

        input_ids = torch.tensor(input_ids, dtype=torch.long)
        attention_mask = torch.tensor(attention_mask, dtype=torch.long)
        if input_ids.shape[1] < self.seq_len:
            pad_len = self.seq_len - input_ids.shape[1]
            pad = torch.full((input_ids.size(0), pad_len), self.pad_token_id, dtype=torch.long)
            input_ids = torch.cat([input_ids, pad], dim=1)
            attention_mask = torch.cat([attention_mask, torch.zeros_like(pad)], dim=1)

        if "labels" in features[0]:
            labels = [f["labels"][: self.seq_len] for f in features]
            labels = torch.tensor(labels, dtype=torch.long)
            if labels.shape[1] < self.seq_len:
                lab_pad = torch.full((labels.size(0), self.seq_len - labels.shape[1]), -100, dtype=torch.long)
                labels = torch.cat([labels, lab_pad], dim=1)
        else:
            labels = input_ids.clone()
            labels[labels == self.pad_token_id] = -100

        return {"input_ids": input_ids, "attention_mask": attention_mask, "labels": labels}

def _rosa_one_sequence_sam_cpu(z_seq: List[int], K: int) -> List[int]:
    import numpy as _np
    T = len(z_seq)
    if T == 0:
        return []

    y = [-1] * T
    c: List[int] = []
    last_token = None
    last_run_of_token: Dict[int, int] = {}

    for t, x in enumerate(z_seq):
        j_last = last_run_of_token.get(x, None)
        if (j_last is not None) and (j_last + 1 < len(c)):
            y[t] = c[j_last + 1]

        if (last_token is None) or (x != last_token):
            c.append(x)
            last_token = x
            last_run_of_token[x] = len(c) - 1

    return y

def _rosa_worker_batch_sample_sam_exact(z_tm_np: "_np.ndarray", K: int) -> "_np.ndarray":
    T, M = z_tm_np.shape
    y_tm = _np.empty((T, M), dtype=_np.int32)
    for m in range(M):
        z_seq = z_tm_np[:, m].tolist()
        y_seq = _rosa_one_sequence_sam_cpu(z_seq, K)
        y_tm[:, m] = _np.asarray(y_seq, dtype=_np.int32)
    return y_tm

def _env_float(k, default):
    try:
        return float(os.environ.get(k, default))
    except Exception:
        return default

_ROSA_POOL: Optional[ProcessPoolExecutor] = None
_ROSA_POOL_PID = None
_ROSA_POOL_LOCK = threading.Lock()

def _rosa_pool_ping() -> int:
    return 0

def _get_rosa_pool() -> ProcessPoolExecutor:
    from concurrent.futures import ProcessPoolExecutor
    import multiprocessing as mp
    global _ROSA_POOL, _ROSA_POOL_PID

    cur_pid = os.getpid()
    with _ROSA_POOL_LOCK:
        need_create = (_ROSA_POOL is None) or (_ROSA_POOL_PID != cur_pid)
        if not need_create:
            try:
                fut = _ROSA_POOL.submit(_rosa_pool_ping)
                fut.result(timeout=0.05)
            except Exception:
                try:
                    _ROSA_POOL.shutdown(wait=False, cancel_futures=True)
                except Exception:
                    pass
                _ROSA_POOL = None
                need_create = True

        if need_create:
            ctx = mp.get_context("spawn")

            oversub = max(1.0, _env_float("ROSA_POOL_OVERSUB", 1.0))
            hard_max = _env_int("ROSA_POOL_MAX_WORKERS", 0)
            req_max  = _env_int("ROSA_CPU_WORKERS", ROSA_CPU_WORKERS)

            logical_cap = int(max(1, _PER_RANK_CPUS * oversub))
            if hard_max > 0:
                logical_cap = min(logical_cap, hard_max)

            max_workers = max(1, min(req_max, logical_cap))

            _ROSA_POOL = ProcessPoolExecutor(
                max_workers=max_workers,
                mp_context=ctx,
                initializer=_rosa_worker_init,
            )
            _ROSA_POOL_PID = cur_pid
    return _ROSA_POOL

@atexit.register
def _shutdown_rosa_pool():
    global _ROSA_POOL
    with _ROSA_POOL_LOCK:
        if _ROSA_POOL is not None:
            try:
                _ROSA_POOL.shutdown(wait=True, cancel_futures=True)
            except Exception:
                pass
            _ROSA_POOL = None

def _rosa_query_sam_exact(z_btm: torch.Tensor, K: int, impl: str = "sam_cpu", pool_getter=_get_rosa_pool) -> torch.Tensor:
    assert z_btm.ndim == 3
    B, T, M = z_btm.shape
    dev = z_btm.device

    if impl != "sam_cpu":
        ys = []
        for b in range(B):
            y_tm = torch.empty((T, M), dtype=torch.int32, device=dev)
            for m in range(M):
                y_seq = _rosa_one_sequence_sam_cuda(z_btm[b, :, m].to(torch.int32), K)
                y_tm[:, m] = y_seq
            ys.append(y_tm)
        y_btm = torch.stack(ys, dim=0).to(torch.int64)
        return y_btm

    pool = pool_getter()
    futs = []
    z_np_list = []
    for b in range(B):
        z_tm_np = z_btm[b].detach().cpu().to(torch.int32).numpy()
        z_np_list.append(z_tm_np)
        futs.append(pool.submit(_rosa_worker_batch_sample_sam_exact, z_tm_np, K))

    y_list = [f.result() for f in futs]
    y_btm = torch.stack([torch.from_numpy(y).to(dev) for y in y_list], dim=0).to(torch.int64)
    return y_btm

def _lcg_route_worker_full_shm(args):
    import numpy as _np
    from multiprocessing import shared_memory

    (T, Kp1,
     shm_name, shm_shape, shm_dtype_str,
     z_seq, y_seq, cand_lists, pos_mask) = args

    shm = shared_memory.SharedMemory(name=shm_name)
    try:
        S_mat = _np.ndarray(tuple(shm_shape), dtype=_np.dtype(shm_dtype_str), buffer=shm.buf)
        row_idx = _np.arange(T, dtype=_np.int64)

        y_old = _np.asarray(y_seq, dtype=_np.int64)
        idx_old = _np.where(y_old >= 0, y_old + 1, 0)
        base_total = float(S_mat[row_idx, idx_old].sum(dtype=_np.float64))

        K = Kp1 - 1
        z_np = _np.asarray(z_seq, dtype=_np.int32)
        if pos_mask is None:
            pos_mask_arr = _np.ones((T,), dtype=_np.bool_)
        else:
            pos_mask_arr = _np.asarray(pos_mask, dtype=_np.bool_)

        cand_arrs = [ _np.asarray(cand_lists[i], dtype=_np.int32) for i in range(T) ]

        deltaL_m = []
        for i in range(T):
            if not pos_mask_arr[i]:
                deltaL_m.append([0.0] * int(cand_arrs[i].shape[0]))
                continue

            zi = int(z_np[i])
            row_vals = []

            for k_cand in cand_arrs[i]:
                kc = int(k_cand)
                if kc == zi:
                    row_vals.append(0.0)
                    continue

                z_prime = z_np.copy()
                z_prime[i] = kc
                y_prime = _rosa_one_sequence_sam_cpu(z_prime.tolist(), K)
                y_prime = _np.asarray(y_prime, dtype=_np.int64)
                idx_new = _np.where(y_prime >= 0, y_prime + 1, 0)

                new_total = float(S_mat[row_idx, idx_new].sum(dtype=_np.float64))
                row_vals.append(new_total - base_total)

            deltaL_m.append(row_vals)

        return deltaL_m
    finally:
        shm.close()

class MultiRouteLCGFunction(torch.autograd.Function):
    @staticmethod
    def forward(ctx,
                v_in: torch.Tensor,
                logits_all: torch.Tensor,
                y_idx: torch.Tensor,
                E_stack: torch.Tensor,
                z_cpu_list: list,
                topk_idx_cpu: list,
                pos_mask_cpu: list | None):
        ctx.save_for_backward(logits_all, y_idx, E_stack)
        ctx.z_cpu_list = z_cpu_list
        ctx.topk_idx_cpu = topk_idx_cpu
        ctx.pos_mask_cpu = pos_mask_cpu
        return v_in

    @staticmethod
    def backward(ctx, grad_v: torch.Tensor):
        (logits_all, y_idx, E_stack) = ctx.saved_tensors
        device = logits_all.device
        dtype  = logits_all.dtype
        B, T, M, K = logits_all.shape
        Kp1 = K + 1

        with torch.no_grad():
            p_all = torch.softmax(logits_all.float(), dim=-1)

        import numpy as _np
        from multiprocessing import shared_memory

        def _create_shm_from_np(arr: _np.ndarray):
            shm = shared_memory.SharedMemory(create=True, size=arr.nbytes)
            buf = _np.ndarray(arr.shape, dtype=arr.dtype, buffer=shm.buf)
            buf[...] = arr
            return shm

        pos_mask_cpu: Optional[List[List[bool]]] = ctx.pos_mask_cpu
        if pos_mask_cpu is None:
            pos_mask_cpu = [[True] * T for _ in range(B)]

        p_topk_vals = []
        for b in range(B):
            p_topk_vals.append([])
            for m in range(M):
                pm = p_all[b, :, m, :]
                idx_topk = torch.tensor(ctx.topk_idx_cpu[b][m], device=pm.device, dtype=torch.long)
                vals = torch.gather(pm, dim=-1, index=idx_topk).cpu().numpy()
                p_topk_vals[b].append(vals)

        pool = _get_rosa_pool()
        futures = []
        shm_list = []

        with torch.no_grad():
            g_no_scale = grad_v.float()
            E_stack_f = E_stack.float()

            for m in range(M):
                E_m = E_stack_f[m]
                S_m = torch.einsum("btd,kd->btk", g_no_scale, E_m).cpu().numpy()

                for b in range(B):
                    shm = _create_shm_from_np(S_m[b])
                    shm_list.append(shm)

                    z_seq = [ctx.z_cpu_list[b][t][m] for t in range(T)]
                    y_seq = []
                    yb = y_idx[b, :, m].cpu().tolist()
                    for yy in yb:
                        y_seq.append(-1 if yy <= 0 else (yy - 1))

                    args = (
                        T, Kp1,
                        shm.name, S_m[b].shape, str(S_m[b].dtype),
                        z_seq,
                        y_seq,
                        ctx.topk_idx_cpu[b][m],
                        pos_mask_cpu[b],
                    )
                    worker = _lcg_route_worker_full_shm
                    futures.append(((b, m, shm), pool.submit(worker, args)))

        deltaL = [[None] * M for _ in range(B)]
        for (b, m, shm), fut in futures:
            deltaL[b][m] = fut.result()
            try:
                shm.close(); shm.unlink()
            except Exception:
                pass

        with torch.no_grad():
            grad_logits = torch.zeros_like(logits_all, dtype=p_all.dtype, device=device)
            for b in range(B):
                for m in range(M):
                    idx_topk = torch.tensor(ctx.topk_idx_cpu[b][m], device=device, dtype=torch.long)
                    deltas   = torch.tensor(deltaL[b][m], device=device, dtype=p_all.dtype)
                    probs    = torch.tensor(p_topk_vals[b][m], device=device, dtype=p_all.dtype)
                    mean_delta = (probs * deltas).sum(dim=-1, keepdim=True)
                    grad_topk  = probs * (deltas - mean_delta)
                    grad_logits[b, :, m, :].scatter_add_(dim=-1, index=idx_topk, src=grad_topk)

        return grad_v, grad_logits.to(dtype), None, None, None, None, None

def patch_qwen3_with_multiroute_rosa(model: Qwen3ForCausalLM):
    base_param = model.model.embed_tokens.weight
    base_dtype = base_param.dtype
    base_device = base_param.device

    hidden_size = model.config.hidden_size
    L = model.config.num_hidden_layers
    M = int(ROSA_NUM_ROUTES)
    K = int(ROSA_VOCAB_SIZE)

    query_impl_env = os.environ.get("ROSA_QUERY_IMPL", "sam_cpu").lower()
    lcg_enable = os.environ.get("LCG_ENABLE", "0") in ("1", "true", "True")

    for li, layer in enumerate(model.model.layers):
        if li == 0:
            continue

        layer.rosa_wlm_list = nn.ModuleList(
            [nn.Linear(hidden_size, K, bias=False).to(dtype=base_dtype, device=base_device) for _ in range(M)]
        )
        for w in layer.rosa_wlm_list:
            nn.init.xavier_uniform_(w.weight)

        layer.rosa_emb_list = nn.ModuleList(
            [nn.Embedding(K + 1, hidden_size).to(dtype=base_dtype, device=base_device) for _ in range(M)]
        )
        for emb in layer.rosa_emb_list:
            nn.init.normal_((emb.weight), mean=0.0, std=0.02)
            with torch.no_grad():
                emb.weight.data[0].zero_()

        layer.rosa_num_routes = M
        layer.rosa_vocab_size = K

        def _forward_with_multiroute_rosa(self: Qwen3DecoderLayer,
                                          hidden_states: torch.Tensor,
                                          attention_mask: Optional[torch.Tensor] = None,
                                          position_ids: Optional[torch.LongTensor] = None,
                                          past_key_values=None,
                                          use_cache: Optional[bool] = False,
                                          cache_position: Optional[torch.LongTensor] = None,
                                          position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
                                          **kwargs):
            residual = hidden_states
            u = self.input_layernorm(hidden_states)

            logits_list, z_list_gpu = [], []
            for head in self.rosa_wlm_list:
                logits_m = F.linear(u.to(head.weight.dtype), head.weight, None).to(u.dtype)
                logits_list.append(logits_m)
                z_list_gpu.append(torch.argmax(logits_m, dim=-1))
            logits_all = torch.stack(logits_list, dim=2)
            z_gpu = torch.stack(z_list_gpu, dim=2)
            B, T, M, K = logits_all.shape

            y_btm = _rosa_query_sam_exact(z_gpu.to(torch.int64), K=K, impl=query_impl_env, pool_getter=_get_rosa_pool)
            y_idx = torch.where(y_btm >= 0, y_btm + 1, torch.zeros_like(y_btm)).to(torch.long)

            attn_out, _ = self.self_attn(
                hidden_states=u, attention_mask=attention_mask,
                position_ids=position_ids, past_key_values=past_key_values,
                use_cache=use_cache, cache_position=cache_position,
                position_embeddings=position_embeddings, **kwargs,
            )

            e_sum = 0; E_stack_gpu = []
            for m, emb in enumerate(self.rosa_emb_list):
                e_m = F.embedding(y_idx[:, :, m], emb.weight)
                e_sum = e_sum + e_m
                E_stack_gpu.append(emb.weight.detach())
            v = (e_sum / float(M)).to(u.dtype)
            E_stack_gpu = torch.stack(E_stack_gpu, dim=0)

            if lcg_enable:
                with torch.no_grad():
                    p_all = F.softmax(logits_all.float(), dim=-1)
                    topk_idx_cpu = []
                    for b in range(B):
                        topk_idx_cpu.append([])
                        for m_idx in range(M):
                            pm = p_all[b, :, m_idx, :]
                            _, inds = torch.topk(pm, k=min(LCG_TOPK, K), dim=-1)
                            topk_idx_cpu[-1].append(inds.cpu().tolist())
                    pos_mask_cpu = None
                    if LCG_POS_SUBSAMPLE < 1.0:
                        mask = (torch.rand((B, T), device=logits_all.device) < LCG_POS_SUBSAMPLE)
                        pos_mask_cpu = mask.cpu().tolist()

                z_btm_list_for_bw = z_gpu.detach().cpu().tolist()
                v = MultiRouteLCGFunction.apply(
                    v, logits_all, y_idx, E_stack_gpu, z_btm_list_for_bw,
                    topk_idx_cpu, pos_mask_cpu
                )

            hidden_states = residual + attn_out + v
            residual = hidden_states
            hidden_states = self.post_attention_layernorm(hidden_states)
            hidden_states = self.mlp(hidden_states)
            hidden_states = residual + hidden_states
            return hidden_states

        layer.forward = _forward_with_multiroute_rosa.__get__(layer, Qwen3DecoderLayer)

    meta = {
        "apply_layers_from": 1,
        "num_routes_per_layer": ROSA_NUM_ROUTES,
        "vocab_per_route": ROSA_VOCAB_SIZE,
    }
    with open(os.path.join(OUTPUT_DIR, "rosa_meta.json"), "w", encoding="utf-8") as f:
        json.dump(meta, f, ensure_ascii=False, indent=2)

def build_model_and_tokenizer() -> Tuple[Qwen3ForCausalLM, AutoTokenizer]:
    config = AutoConfig.from_pretrained(MODEL_LOCAL_DIR)
    config.sliding_window = ATTN_WINDOW
    config.max_window_layers = FIRST_GLOBAL_LAYERS
    if (not hasattr(config, "layer_types")) or (config.layer_types is None):
        config.layer_types = [
            "full_attention" if i < config.max_window_layers else "sliding_attention"
            for i in range(config.num_hidden_layers)
        ]
    if hasattr(config, "attn_implementation"):
        config.attn_implementation = "flash_attention_2" if USE_FLASH_ATTN else "sdpa"
    else:
        config._attn_implementation = "flash_attention_2" if USE_FLASH_ATTN else "sdpa"

    tokenizer = AutoTokenizer.from_pretrained(MODEL_LOCAL_DIR, use_fast=True)
    model = Qwen3ForCausalLM.from_pretrained(
        MODEL_LOCAL_DIR,
        config=config,
        torch_dtype=torch.bfloat16 if BF16 else torch.float16,
        low_cpu_mem_usage=True,
    )

    if GRADIENT_CHECKPOINTING:
        model.gradient_checkpointing_enable()
    else:
        model.gradient_checkpointing_disable()

    model.config.use_cache = False

    patch_qwen3_with_multiroute_rosa(model)
    return model, tokenizer

def save_rosa_only(model: Qwen3ForCausalLM, out_dir: str):
    state = {}
    for i, layer in enumerate(model.model.layers):
        if hasattr(layer, "rosa_wlm_list"):
            for m, head in enumerate(layer.rosa_wlm_list):
                state[f"model.layers.{i}.rosa_wlm_list.{m}.weight"] = head.weight.detach().cpu()
        if hasattr(layer, "rosa_emb_list"):
            for m, emb in enumerate(layer.rosa_emb_list):
                state[f"model.layers.{i}.rosa_emb_list.{m}.weight"] = emb.weight.detach().cpu()
    path = os.path.join(out_dir, SAVE_STATE_DICT_NAME)
    torch.save(state, path)
    print(f"[save] saved ROSA-only params to: {path}")

def build_optimizer_params(model):
    wlm_params, emb_params, backbone_params = [], [], []
    for n, p in model.named_parameters():
        if "rosa_wlm_list" in n:
            wlm_params.append(p)
        elif "rosa_emb_list" in n:
            emb_params.append(p)
        else:
            backbone_params.append(p)

    param_groups = [
        {"params": wlm_params, "lr": LR_ROSA, "weight_decay": WEIGHT_DECAY},
        {"params": emb_params, "lr": LR_ROSA, "weight_decay": 0.0},
    ]
    if LR_BACKBONE and LR_BACKBONE > 0.0:
        no_decay, has_decay = [], []
        for n, p in model.named_parameters():
            if ("rosa_" in n):
                continue
            if any(k in n.lower() for k in ["bias", "norm", "layernorm", "ln"]):
                no_decay.append(p)
            else:
                has_decay.append(p)
        param_groups += [
            {"params": has_decay, "lr": LR_BACKBONE, "weight_decay": WEIGHT_DECAY},
            {"params": no_decay, "lr": LR_BACKBONE, "weight_decay": 0.0},
        ]
    else:
        for p in backbone_params:
            p.requires_grad_(False)

    return param_groups

from transformers.trainer_callback import TrainerCallback

def is_main_process() -> bool:
    return _env_int("RANK", 0) == 0

class RosaZeroRowCallback(TrainerCallback):
    def on_init_end(self, args, state, control, model=None, **kwargs):
        if model is not None:
            with torch.no_grad():
                for layer in model.model.layers:
                    if hasattr(layer, "rosa_emb_list"):
                        for emb in layer.rosa_emb_list:
                            emb.weight.data[0].zero_()
        return control

    def on_step_end(self, args, state, control, model=None, **kwargs):
        if model is not None:
            with torch.no_grad():
                for layer in model.model.layers:
                    if hasattr(layer, "rosa_emb_list"):
                        for emb in layer.rosa_emb_list:
                            emb.weight.data[0].zero_()
        return control

def main():
    set_seed(SEED)

    raw = load_from_disk(DATASET_DIR)
    train_ds = raw["train"]
    test_ds = raw.get("test", raw["validation"] if "validation" in raw else None)
    assert test_ds is not None, "需要存在 test 或 validation split"

    model, tokenizer = build_model_and_tokenizer()
    pad_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id
    data_collator = FixedLenLMCollator(pad_token_id=pad_id, seq_len=SEQ_LEN)

    training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        num_train_epochs=NUM_EPOCHS,
        per_device_train_batch_size=PER_DEVICE_TRAIN_BSZ,
        per_device_eval_batch_size=1,
        gradient_accumulation_steps=GRAD_ACCUM_STEPS,
        learning_rate=LR_ROSA,
        warmup_steps=WARMUP_STEPS,
        logging_steps=LOGGING_STEPS,
        eval_steps=EVAL_STEPS,
        save_strategy="no",
        report_to="none",
        fp16=(not BF16) and torch.cuda.is_available(),
        bf16=BF16,
        dataloader_num_workers=2,
        gradient_checkpointing=GRADIENT_CHECKPOINTING,
        remove_unused_columns=False,
        optim="adamw_torch",
    )

    optimizer_params = build_optimizer_params(model)

    class _Trainer(Trainer):
        def create_optimizer(self):
            if self.optimizer is None:
                self.optimizer = torch.optim.AdamW(optimizer_params, betas=(0.9, 0.98), eps=1e-8)
            return self.optimizer

    trainer = _Trainer(
        model=model,
        args=training_args,
        train_dataset=train_ds,
        eval_dataset=test_ds,
        data_collator=data_collator,
        tokenizer=tokenizer,
        callbacks=[RosaZeroRowCallback()],
    )

    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

    print(f"总参数:        {total_params:,}")

    trainer.train()
    metrics = trainer.evaluate()

    meta = {
        "model_local_dir": MODEL_LOCAL_DIR,
        "dataset_dir": DATASET_DIR,
        "seq_len": SEQ_LEN,
        "attn_window": ATTN_WINDOW,
        "first_global_layers": FIRST_GLOBAL_LAYERS,
        "rosa": {
            "num_routes": ROSA_NUM_ROUTES,
            "vocab_size": ROSA_VOCAB_SIZE,
            "lcg_topk": LCG_TOPK,
            "pos_subsample": LCG_POS_SUBSAMPLE,
            "lr_rosa": LR_ROSA, "lr_backbone": LR_BACKBONE,
            "k_per_layer": {str(i): (getattr(model.model.layers[i], "rosa_vocab_size", 0))
                            for i in range(model.config.num_hidden_layers)}
        },
        "metrics": metrics,
        "time": time.asctime(),
    }

    if is_main_process():
        print("[eval] metrics:", metrics)
        save_rosa_only(model, OUTPUT_DIR)
        with open(os.path.join(OUTPUT_DIR, "run_meta.json"), "w", encoding="utf-8") as f:
            json.dump(meta, f, ensure_ascii=False, indent=2)
        print(f"[done] meta saved at {os.path.join(OUTPUT_DIR, 'run_meta.json')}")

if __name__ == "__main__":
    main()
