import os
import math
import json
import time
import atexit
from dataclasses import dataclass
from typing import Dict, List, Optional, Tuple

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch import Tensor

import numpy as np

from concurrent.futures import ProcessPoolExecutor, as_completed
import multiprocessing as mp

from datasets import load_from_disk
from transformers import (
    AutoConfig,
    AutoTokenizer,
    Trainer,
    TrainingArguments,
    set_seed,
)
from transformers.models.qwen3.modeling_qwen3 import (
    Qwen3ForCausalLM,
    Qwen3DecoderLayer,
)

import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor
import threading
import os
import atexit

# =========================
# 全局变量（路径 / 设备 / 超参）
# =========================
# ---- defaults: enabled unless explicitly disabled ----
import os as _os
_os.environ.setdefault("ROSA_USE_NUMBA", "1")
_os.environ.setdefault("ROSA_NUMBA_PARALLEL", "1")
_os.environ.setdefault("LCG_ENABLE", "1")

# flags (respect explicit 0/false)
_USE_NUMBA = _os.environ.get("ROSA_USE_NUMBA", "1").lower() not in ("0", "false")
_PARALLEL  = _os.environ.get("ROSA_NUMBA_PARALLEL", "1").lower() not in ("0", "false")

# try numba; fall back cleanly if unavailable
try:
    import numba as _nb
    _NUMBA_OK = _USE_NUMBA  # only true if user didn't disable
except Exception:
    _NUMBA_OK = False


# --- 资源路径 ---
MODEL_LOCAL_DIR = "/lpai/volumes/base-base-ali-sh-mix/zhengyunao/zhengyunao/stage6/Qwen__Qwen3-0.6B/main/"
DATASET_DIR     = "/lpai/volumes/base-base-ali-sh-mix/zhengyunao/zhengyunao/stage6/emozilla__pg19/23-10-09-1506/tokenized_qwen4k/"
OUTPUT_DIR      = "/lpai/volumes/base-base-ali-sh-mix/zhengyunao/zhengyunao/stage6/10.13rosa/rosa_qwen3_0_6B_pg19_multiroute_lcg"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# --- 训练与模型 ---
USE_FLASH_ATTN = True
BF16 = True if torch.cuda.is_available() else False

SEQ_LEN = 4096
ATTN_WINDOW = 1024
FIRST_GLOBAL_LAYERS = 1  # 第 0 层保留全局注意

# --- ROSA（多路）与 LCG 超参（全部通过全局变量配置） ---
ROSA_NUM_ROUTES: int = 4        # 每层路数 M（几十路）
ROSA_VOCAB_SIZE: int = 64       # 每路词表 K（含 0 作为“无命中”的额外行在 Embedding 里体现为 K+1）
LCG_TOPK: int = 16                # 每位置候选 Top-K（建议 8~16）
LCG_POS_SUBSAMPLE: float = 1.0   # 位置子采样比率（1.0=全位置；可设 0.25/0.5 稳控负载）
ROSA_CPU_WORKERS: int = 128  # 每 rank CPU worker 数


# --- 优化 / 训练 ---
LR_ROSA = 1e-3
LR_BACKBONE = 0.0               # 0 => 冻结主干
WEIGHT_DECAY = 0.01
WARMUP_STEPS = 20
NUM_EPOCHS = 1
PER_DEVICE_TRAIN_BSZ = 1
GRAD_ACCUM_STEPS = 1
LOGGING_STEPS = 20
EVAL_STEPS = 200
SEED = 42

SAVE_STATE_DICT_NAME = "rosa_adapters.pt"

# ### 冻结主干 ⇒ 建议关闭 gradient checkpointing（避免无 grad 输入触发报错）
GRADIENT_CHECKPOINTING = bool(LR_BACKBONE and LR_BACKBONE > 0.0)

_ROSA_POOL = None             # type: ProcessPoolExecutor | None
_ROSA_POOL_PID = None         # 记录创建该池的进程 PID，PID 变了就重建
_ROSA_POOL_LOCK = threading.Lock()

# =========================
# 环境信息 & 每 rank CPU 配额
# =========================

def _env_int(k, default):
    try:
        return int(os.environ.get(k, default))
    except Exception:
        return default

LOCAL_WORLD_SIZE = _env_int("LOCAL_WORLD_SIZE", max(1, torch.cuda.device_count()))
LOCAL_RANK = _env_int("LOCAL_RANK", 0)
GLOBAL_RANK = _env_int("RANK", 0)

# === 新增：NVTX（可选，便于 Nsight 观测） ===
try:
    import torch.cuda.nvtx as nvtx
except Exception:
    class _DummyNVTX:
        def range_push(self, *a, **k): pass
        def range_pop(self): pass
    nvtx = _DummyNVTX()

# === 新增：线程池用于同进程异步 CPU 计算（与 GPU 重叠） ===
from concurrent.futures import ThreadPoolExecutor
_ROSA_THREAD_POOL = ThreadPoolExecutor(
    max_workers=max(1, _env_int("ROSA_THREAD_WORKERS", 2))
)

def _wait_event(ev: "torch.cuda.Event"):
    """阻塞当前线程直至 CUDA 事件完成（不阻塞 GPU 流）"""
    try:
        ev.synchronize()
    except Exception:
        pass



def _available_logical_cpus() -> int:
    """优先用进程 CPU 亲和性 / cgroup 限制，最后退回 os.cpu_count()。"""
    # 1) 亲和性（Linux）
    try:
        if hasattr(os, "sched_getaffinity"):
            return max(1, len(os.sched_getaffinity(0)))
    except Exception:
        pass

    # 2) cgroup v1/v2 的 cpuset（容器常见）
    def _parse_cpuset(s: str) -> int:
        # "0-3,8,10-11" -> 4 + 1 + 2 = 7
        n = 0
        for part in s.split(","):
            part = part.strip()
            if not part:
                continue
            if "-" in part:
                a, b = part.split("-")
                n += int(b) - int(a) + 1
            else:
                n += 1
        return n

    try:
        for p in ("/sys/fs/cgroup/cpuset.cpus", "/sys/fs/cgroup/cpuset/cpuset.cpus"):
            if os.path.exists(p):
                with open(p) as f:
                    txt = f.read().strip()
                if txt:
                    return max(1, _parse_cpuset(txt))
    except Exception:
        pass

    # 3) 兜底
    return max(1, os.cpu_count() or 1)

# === 用这个函数来定义每 rank CPU 上限 ===
_TOTAL_CPUS = _available_logical_cpus()
_PER_RANK_CPUS = max(1, _TOTAL_CPUS // max(1, LOCAL_WORLD_SIZE))


# 避免二级超订阅
torch.set_num_threads(_PER_RANK_CPUS)


def _rosa_worker_init():
    # 子进程内禁用多线程BLAS，防止二次超订阅
    os.environ.setdefault("OMP_NUM_THREADS", "1")
    os.environ.setdefault("MKL_NUM_THREADS", "1")
    try:
        import torch as _t
        _t.set_num_threads(1)
    except Exception:
        pass

import numpy as _np
import torch

def to_c_np(t: torch.Tensor, dtype=_np.int32):
    """
    安全把 torch.Tensor → C 连续 numpy.ndarray（尽量零拷贝）。
    dtype: 目标 numpy dtype（默认 int32）。
    """
    return _np.asarray(
        t.detach().to(torch.int32 if dtype is _np.int32 else torch.bool).contiguous().cpu().numpy(),
        order="C",
        dtype=dtype,
    )

def to_c_bool_np(t: torch.Tensor):
    """torch.bool/byte → C 连续 numpy.bool_"""
    return _np.asarray(t.detach().to(torch.bool).contiguous().cpu().numpy(), order="C", dtype=_np.bool_)



class _SAMFoldedCPU:
    """
    折叠串 SAM（RLE + Longest&Latest）：
    - next: [S,K] 转移，-1 表示无。
    - link: suffix-link。
    - length: 状态的 maxlen。
    - e: 右端位置集合的右极值（折叠串下标），用于“最近”。
    - last: 当前折叠串 c 的 last 状态。
    - c: 折叠后的符号序列（仅保存每个 run 的首符号）。
    - last_sym: 上一个原始符号（用于判断是否新 run）。
    """
    __slots__ = ("next", "link", "length", "e", "last", "size", "K", "c", "last_sym")

    def __init__(self, max_states: int, K: int):
        self.K = int(K)
        S = int(max_states)
        self.next   = _np.full((S, self.K), -1, dtype=_np.int32)
        self.link   = _np.full((S,),       -1, dtype=_np.int32)
        self.length = _np.zeros((S,),          dtype=_np.int32)
        self.e      = _np.full((S,),       -1, dtype=_np.int32)
        self.last   = 0
        self.size   = 1
        self.c: List[int] = []
        self.last_sym: Optional[int] = None

    def _new_state(self, L: int) -> int:
        s = self.size
        self.size += 1
        self.length[s] = L
        self.link[s] = -1
        self.e[s] = -1
        # 注意：next[s,:] 在 extend 时按需填，不用清零
        self.next[s, :].fill(-1)
        return s

    # === 查询：在不改索引的前提下匹配“最长可接受 x”的状态 ===
    def match_next(self, x: int) -> int:
        p = self.last
        nxt = self.next
        link = self.link
        while p != -1 and nxt[p, x] == -1:
            p = link[p]
        return -1 if p == -1 else int(nxt[p, x])

    # === 读取“最近后继的不同 token” ===
    def nextdiff_from_state(self, q: int) -> int:
        if q == -1:
            return -1
        rpos = int(self.e[q])          # 折叠串右端位置
        nxt = rpos + 1
        if 0 <= rpos and nxt < len(self.c):
            # 折叠串天然保证 c[nxt] != c[rpos]
            return int(self.c[nxt])
        return -1

    # === 仅在 run 变更时 extend（提交阶段）===
    def extend_run(self, x: int):
        # 追加到折叠串
        self.c.append(x)
        pos = len(self.c) - 1

        # 标准 SAM extend
        last = self.last
        nxt = self.next
        link = self.link
        length = self.length
        e = self.e

        cur = self._new_state(length[last] + 1)
        p = last
        while p != -1 and nxt[p, x] == -1:
            nxt[p, x] = cur
            p = link[p]
        if p == -1:
            link[cur] = 0
        else:
            q = int(nxt[p, x])
            if length[p] + 1 == length[q]:
                link[cur] = q
            else:
                clone = self._new_state(length[p] + 1)
                nxt[clone, :] = nxt[q, :]
                link[clone]   = link[q]
                e[clone]      = e[q]
                while p != -1 and nxt[p, x] == q:
                    nxt[p, x] = clone
                    p = link[p]
                link[q]   = clone
                link[cur] = clone

        # 回填右端位：把“最近结束位 = 当前 pos”沿 suffix‑link 向上推
        v = cur
        while v != -1 and e[v] != pos:
            e[v] = pos
            v = link[v]

        self.last = cur
        self.last_sym = x

    # === 高层接口：先查询，再按需提交（严格“查询不改索引”）===
    def query_then_commit(self, x: int) -> int:
        q = self.match_next(x)
        a = self.nextdiff_from_state(q)
        # 仅在新 run 时提交
        if self.last_sym is None or x != self.last_sym:
            self.extend_run(x)
        return a




# ======================================================
# 数据集整理
# ======================================================
@dataclass
class FixedLenLMCollator:
    pad_token_id: int
    seq_len: int

    def __call__(self, features):
        input_ids = [f["input_ids"][: self.seq_len] for f in features]
        if "attention_mask" in features[0]:
            attention_mask = [f["attention_mask"][: self.seq_len] for f in features]
        else:
            attention_mask = [[1] * len(x) for x in input_ids]

        input_ids = torch.tensor(input_ids, dtype=torch.long)
        attention_mask = torch.tensor(attention_mask, dtype=torch.long)
        if input_ids.shape[1] < self.seq_len:
            pad_len = self.seq_len - input_ids.shape[1]
            pad = torch.full((input_ids.size(0), pad_len), self.pad_token_id, dtype=torch.long)
            input_ids = torch.cat([input_ids, pad], dim=1)
            attention_mask = torch.cat([attention_mask, torch.zeros_like(pad)], dim=1)

        if "labels" in features[0]:
            labels = [f["labels"][: self.seq_len] for f in features]
            labels = torch.tensor(labels, dtype=torch.long)
            if labels.shape[1] < self.seq_len:
                lab_pad = torch.full((labels.size(0), self.seq_len - labels.shape[1]), -100, dtype=torch.long)
                labels = torch.cat([labels, lab_pad], dim=1)
        else:
            labels = input_ids.clone()
            labels[labels == self.pad_token_id] = -100

        return {"input_ids": input_ids, "attention_mask": attention_mask, "labels": labels}









# --------- 多进程池（稳，按每 rank CPU 绑限） ---------
# === 新增：读取 float 环境变量 ===
def _env_float(k, default):
    try:
        return float(os.environ.get(k, default))
    except Exception:
        return default

# --------- 多进程池（稳，按每 rank CPU 绑限 + 可控超订阅） ---------
_ROSA_POOL: Optional[ProcessPoolExecutor] = None
_ROSA_POOL_PID = None
_ROSA_POOL_LOCK = threading.Lock()

def _rosa_pool_ping() -> int:
    return 0

def _get_rosa_pool() -> ProcessPoolExecutor:
    """
    绑定到 _PER_RANK_CPUS，同时允许通过 ROSA_POOL_OVERSUB 做“可控超订阅”，
    以避免在多 GPU 场景下每 rank 只有 1~2 个 worker 导致 CPU 大量空闲。
    - 环境变量：
        ROSA_CPU_WORKERS: 目标上限（原有，默认 128）
        ROSA_POOL_OVERSUB: 相对 _PER_RANK_CPUS 的放大倍数（默认 1.0；可设 2~6）
        ROSA_POOL_MAX_WORKERS: 硬上限（可选，默认 0 表示不启用）
    """
    from concurrent.futures import ProcessPoolExecutor
    import multiprocessing as mp
    global _ROSA_POOL, _ROSA_POOL_PID

    cur_pid = os.getpid()
    with _ROSA_POOL_LOCK:
        need_create = (_ROSA_POOL is None) or (_ROSA_POOL_PID != cur_pid)
        if not need_create:
            try:
                fut = _ROSA_POOL.submit(_rosa_pool_ping)
                fut.result(timeout=0.05)
            except Exception:
                try:
                    _ROSA_POOL.shutdown(wait=False, cancel_futures=True)
                except Exception:
                    pass
                _ROSA_POOL = None
                need_create = True

        if need_create:
            ctx = mp.get_context("spawn")

            # 1) 读取并行度参数
            oversub = max(1.0, _env_float("ROSA_POOL_OVERSUB", 1.0))
            hard_max = _env_int("ROSA_POOL_MAX_WORKERS", 0)
            req_max  = _env_int("ROSA_CPU_WORKERS", ROSA_CPU_WORKERS)

            # 2) 基于每 rank CPU 配额的可控放大
            logical_cap = int(max(1, _PER_RANK_CPUS * oversub))
            if hard_max > 0:
                logical_cap = min(logical_cap, hard_max)

            max_workers = max(1, min(req_max, logical_cap))

            _ROSA_POOL = ProcessPoolExecutor(
                max_workers=max_workers,
                mp_context=ctx,
                initializer=_rosa_worker_init,   # 仍然在子进程里禁用 OMP/MKL 线程
            )
            _ROSA_POOL_PID = cur_pid
    return _ROSA_POOL


@atexit.register
def _shutdown_rosa_pool():
    global _ROSA_POOL
    with _ROSA_POOL_LOCK:
        if _ROSA_POOL is not None:
            try:
                _ROSA_POOL.shutdown(wait=True, cancel_futures=True)
            except Exception:
                pass
            _ROSA_POOL = None

def _rosa_worker_batch_btm_sam_exact(z_btm_np: "_np.ndarray", K: int) -> "_np.ndarray":
    """
    整批 ROSA 查询（折叠串 + 最长且最近），一次性处理 [B,T,M]。
    入:  z_btm_np [B,T,M] int32
    出:  y_btm_np [B,T,M] int32 （-1 或 0..K-1）
    """
    B, T, M = z_btm_np.shape
    y_btm = _np.empty((B, T, M), dtype=_np.int32)
    for b in range(B):
        y_btm[b] = _rosa_worker_batch_sample_sam_exact(z_btm_np[b], K)
    return y_btm



# -------------------------
# 工具：RLE 计数（折叠后长度）
# -------------------------
def _count_runs(z):
    if not z:
        return 0
    arr = _np.asarray(z, dtype=_np.int32)
    return int(1 + _np.count_nonzero(arr[1:] != arr[:-1]))

# -------------------------
# Numba 核心（可并行）
# -------------------------
if _NUMBA_OK:
    @(_nb.njit(cache=True, fastmath=False))
    def _count_runs_nb(arr):
        T = arr.shape[0]
        if T == 0: 
            return 0
        cnt = 1
        last = arr[0]
        for i in range(1, T):
            if arr[i] != last:
                cnt += 1
                last = arr[i]
        return cnt

    @(_nb.njit(cache=True, fastmath=False, inline='always'))
    def _sam_new_state(next_arr, link, length, e, size, L):
        s = size[0]
        size[0] = s + 1
        length[s] = L
        link[s]   = -1
        e[s]      = -1
        # next[s,:] 初始化为 -1
        for j in range(next_arr.shape[1]):
            next_arr[s, j] = -1
        return s

    @(_nb.njit(cache=True, fastmath=False))
    def _sam_extend(next_arr, link, length, e, last_arr, size, x, pos):
        """标准 SAM extend，随后沿 suffix-link 回填 e[v]=pos；last_arr[0] 更新为 cur。"""
        last = last_arr[0]
        cur = _sam_new_state(next_arr, link, length, e, size, length[last] + 1)
        p = last
        K = next_arr.shape[1]

        while p != -1 and next_arr[p, x] == -1:
            next_arr[p, x] = cur
            p = link[p]

        if p == -1:
            link[cur] = 0
        else:
            q = next_arr[p, x]
            if length[p] + 1 == length[q]:
                link[cur] = q
            else:
                clone = _sam_new_state(next_arr, link, length, e, size, length[p] + 1)
                # 复制转移
                for j in range(K):
                    next_arr[clone, j] = next_arr[q, j]
                link[clone] = link[q]
                e[clone]    = e[q]
                while p != -1 and next_arr[p, x] == q:
                    next_arr[p, x] = clone
                    p = link[p]
                link[q]   = clone
                link[cur] = clone

        # 回填最近右端位
        v = cur
        while v != -1 and e[v] != pos:
            e[v] = pos
            v = link[v]

        last_arr[0] = cur

    @(_nb.njit(cache=True, fastmath=False, inline='always'))
    def _sam_match_next(next_arr, link, last_state, x):
        p = last_state
        while p != -1 and next_arr[p, x] == -1:
            p = link[p]
        if p == -1:
            return -1
        return next_arr[p, x]

    @(_nb.njit(cache=True, fastmath=False))
    def _rosa_seq_folded_nb(z, K):
        """
        单路：折叠串 + SAM（最长且最近）。查询阶段不改索引；仅在新 run 时 extend。
        输入 z[int32,T], 输出 y[int32,T] ∈ {-1, 0..K-1}
        """
        T = z.shape[0]
        y = _np.empty((T,), dtype=_np.int32)
        if T == 0:
            return y

        # 预估状态上界：2R+5
        R = _count_runs_nb(z)
        S_cap = 2 * R + 5
        next_arr = _np.empty((S_cap, K), dtype=_np.int32)
        link     = _np.empty((S_cap,),   dtype=_np.int32)
        length   = _np.empty((S_cap,),   dtype=_np.int32)
        e        = _np.empty((S_cap,),   dtype=_np.int32)
        # 初始化根
        for j in range(K):
            next_arr[0, j] = -1
        link[0]   = -1
        length[0] = 0
        e[0]      = -1
        size = _np.empty((1,), dtype=_np.int32)
        size[0] = 1
        last_arr = _np.empty((1,), dtype=_np.int32)
        last_arr[0] = 0

        # 折叠串缓冲区（最大长度 <= T）
        c = _np.empty((T,), dtype=_np.int32)
        c_len = 0
        last_sym = -2147483647  # sentinel

        for t in range(T):
            x = z[t]
            # 查询阶段
            q = _sam_match_next(next_arr, link, last_arr[0], x)
            if q == -1:
                y[t] = -1
            else:
                rpos = e[q]
                nxt  = rpos + 1
                if 0 <= rpos and nxt < c_len:
                    y[t] = c[nxt]  # 折叠串天然保证相邻不同
                else:
                    y[t] = -1
            # 提交阶段：仅在新 run 时 extend
            if t == 0 or x != last_sym:
                # push to folded c
                c[c_len] = x
                c_len += 1
                _sam_extend(next_arr, link, length, e, last_arr, size, x, c_len - 1)
                last_sym = x

        return y

    @(_nb.njit(cache=True, fastmath=False))
    def _rosa_batch_btm_nb(z_btm, K):
        """
        批量 [B,T,M] → [B,T,M] 的 y（-1 或 0..K-1），每个 (b,m) 独立运行 _rosa_seq_folded_nb。
        若开启 parallel=True，则对 b 维并行。
        """
        B, T, M = z_btm.shape
        y_btm = _np.empty((B, T, M), dtype=_np.int32)
        if _PARALLEL:
            for b in _nb.prange(B):
                for m in range(M):
                    y_btm[b, :, m] = _rosa_seq_folded_nb(z_btm[b, :, m], K)
        else:
            for b in range(B):
                for m in range(M):
                    y_btm[b, :, m] = _rosa_seq_folded_nb(z_btm[b, :, m], K)
        return y_btm

    @(_nb.njit(cache=True, fastmath=False))
    def _lcg_index_level_seq_nb(z, K, cand_tm, pos_mask):
        """
        单路 Index‑LCG：在折叠索引视图内做“改一步”的查询，不改显式序列。
        z[int32,T], cand_tm[int32,T,topk], pos_mask[bool,T]（可全 True）
        返回 y_cf[int16,T,topk] ∈ {-2(跳过), -1, 0..K-1}
        """
        T = z.shape[0]
        topk = cand_tm.shape[1]
        out = _np.empty((T, topk), dtype=_np.int16)
        if T == 0:
            return out

        # 预建 SAM（同 _rosa_seq_folded_nb）
        R = _count_runs_nb(z)
        S_cap = 2 * R + 5
        next_arr = _np.empty((S_cap, K), dtype=_np.int32)
        link     = _np.empty((S_cap,),   dtype=_np.int32)
        length   = _np.empty((S_cap,),   dtype=_np.int32)
        e        = _np.empty((S_cap,),   dtype=_np.int32)
        for j in range(K):
            next_arr[0, j] = -1
        link[0]   = -1
        length[0] = 0
        e[0]      = -1
        size = _np.empty((1,), dtype=_np.int32); size[0] = 1
        last_arr = _np.empty((1,), dtype=_np.int32); last_arr[0] = 0
        c = _np.empty((T,), dtype=_np.int32); c_len = 0
        last_sym = -2147483647

        for t in range(T):
            if not pos_mask[t]:
                for j in range(topk):
                    out[t, j] = -2  # 跳过
            else:
                # 对本步候选集，仅在索引上查询（不提交）
                for j in range(topk):
                    k = cand_tm[t, j]
                    q = _sam_match_next(next_arr, link, last_arr[0], k)
                    if q == -1:
                        out[t, j] = -1
                    else:
                        rpos = e[q]; nxt = rpos + 1
                        if 0 <= rpos and nxt < c_len:
                            out[t, j] = _np.int16(c[nxt])
                        else:
                            out[t, j] = -1
            # 提交真实 z[t]（仅在新 run）
            x = z[t]
            if t == 0 or x != last_sym:
                c[c_len] = x; c_len += 1
                _sam_extend(next_arr, link, length, e, last_arr, size, x, c_len - 1)
                last_sym = x

        return out

    @(_nb.njit(cache=True, fastmath=False))
    def _lcg_index_level_bmt_nb(z_btm, K, cand_bmtk, pos_mask_bt):
        """
        批量 Index‑LCG：z[B,T,M], cand[B,M,T,topk], mask[B,T] → y_cf[B,M,T,topk]
        """
        B, T, M = z_btm.shape
        topk = cand_bmtk.shape[3]
        out = _np.empty((B, M, T, topk), dtype=_np.int16)
        if _PARALLEL:
            for b in _nb.prange(B):
                for m in range(M):
                    out[b, m] = _lcg_index_level_seq_nb(z_btm[b, :, m], K, cand_bmtk[b, m], pos_mask_bt[b])
        else:
            for b in range(B):
                for m in range(M):
                    out[b, m] = _lcg_index_level_seq_nb(z_btm[b, :, m], K, cand_bmtk[b, m], pos_mask_bt[b])
        return out

# -------------------------
# 包装器：保持外部接口不变
# -------------------------
def _rosa_one_sequence_sam_cpu(z_seq, K: int):
    """单条序列（Python API 保持不变）。"""
    if _NUMBA_OK:
        z = _np.asarray(z_seq, dtype=_np.int32)
        y = _rosa_seq_folded_nb(z, int(K))
        return [int(v) for v in y]
    # --- 回退：Numpy 版（与你之前的等价） ---
    T = len(z_seq)
    if T == 0: return []
    R = _count_runs(z_seq)
    S_cap = max(4, 2 * R + 5)
    # 复用你之前的 _SAMFoldedCPU 实现（若已删除，可保留你旧版 _SAMExactCPU）
    sam = _SAMFoldedCPU(max_states=S_cap, K=K)
    y = []
    last = None
    for t in range(T):
        x = int(z_seq[t])
        a = sam.nextdiff_from_state(sam.match_next(x))
        y.append(-1 if a is None else int(a))
        if (last is None) or (x != last):
            sam.extend_run(x); last = x
    return y

def _rosa_worker_batch_sample_sam_exact(z_tm_np: "_np.ndarray", K: int) -> "_np.ndarray":
    """保持签名：输入 [T,M] → 输出 [T,M]。"""
    if _NUMBA_OK:
        T, M = z_tm_np.shape
        z_btm = z_tm_np.reshape(1, T, M)
        y_btm = _rosa_batch_btm_nb(z_btm, int(K))
        return y_btm[0]
    # 回退：逐路循环
    T, M = z_tm_np.shape
    y_tm = _np.empty((T, M), dtype=_np.int32)
    for m in range(M):
        y_tm[:, m] = _np.asarray(_rosa_one_sequence_sam_cpu(z_tm_np[:, m].tolist(), K), dtype=_np.int32)
    return y_tm

def _rosa_query_sam_exact(z_btm: torch.Tensor, K: int, impl: str = "sam_cpu", pool_getter=_get_rosa_pool) -> torch.Tensor:
    """
    z_btm: [B,T,M] int64/CPU|CUDA → y_btm: [B,T,M] int64（-1 或 0..K-1）
    语义：折叠串 + SAM（最长且最近）；查询阶段不改索引；仅在新 run 时 extend。
    """
    assert z_btm.ndim == 3
    B, T, M = z_btm.shape
    device = z_btm.device

    if impl != "sam_cpu":
        # 兼容路径（慢），只用于调试
        ys = []
        for b in range(B):
            y_tm = torch.empty((T, M), dtype=torch.int64, device=device)
            for m in range(M):
                y_seq = _rosa_one_sequence_sam_cpu(z_btm[b, :, m].to(torch.int32).tolist(), K)
                y_tm[:, m] = torch.tensor(y_seq, dtype=torch.int64, device=device)
            ys.append(y_tm)
        return torch.stack(ys, dim=0)

    # --- CPU 并行 / Numba 路径 ---
    # 原来出错：z_btm.detach().to(torch.int32).cpu().numpy(order="C")
    z_np = to_c_np(z_btm, dtype=_np.int32)  # [B,T,M] C-contiguous

    # 如果你使用了 Numba 版，这里可以走 Numba 内核；否则回退进程池
    if '_rosa_batch_btm_nb' in globals() and _NUMBA_OK:
        y_np = _rosa_batch_btm_nb(z_np, int(K))
        return torch.from_numpy(y_np).to(device=device, dtype=torch.int64, non_blocking=True)

    pool = pool_getter()
    fut = pool.submit(_rosa_worker_batch_btm_sam_exact, z_np, K)
    y_np = fut.result()
    return torch.from_numpy(y_np).to(device=device, dtype=torch.int64)


def _lcg_route_worker_index_level_sam(args):
    """
    单路 Index‑LCG 反事实（保持签名不变）：返回 [T][topk] 候选的新 y_t(k)。
    """
    (T, K, z_seq, cand_lists, pos_mask) = args
    if _NUMBA_OK:
        z = _np.asarray(z_seq, dtype=_np.int32)
        topk = len(cand_lists[0]) if T > 0 else 0
        cand = _np.empty((T, topk), dtype=_np.int32)
        for t in range(T):
            row = cand_lists[t]
            for j in range(topk):
                cand[t, j] = int(row[j])
        if pos_mask is None:
            mask = _np.ones((T,), dtype=_np.bool_)
        else:
            mask = _np.asarray(pos_mask, dtype=_np.bool_)
        ycf = _lcg_index_level_seq_nb(z, int(K), cand, mask)   # int16[T,topk]
        # 还原成 List[List[int]]，保持之前接口
        out = []
        for i in range(T):
            row = [_np.int32(ycf[i, j]).item() for j in range(cand.shape[1])]
            out.append(row)
        return out

    # --- 回退：原 Numpy 实现（可用你之前的版本） ---
    R = _count_runs(z_seq)
    S_cap = max(4, 2 * R + 5)
    sam = _SAMFoldedCPU(max_states=S_cap, K=K)
    y_cf_lists = []
    last = None
    for t in range(T):
        x = int(z_seq[t])
        cands = cand_lists[t]
        if (pos_mask is not None) and (not pos_mask[t]):
            y_cf_lists.append([-2] * len(cands))
        else:
            row = []
            for k in cands:
                q = sam.match_next(int(k))
                a = sam.nextdiff_from_state(q)
                row.append(-1 if a is None else int(a))
            y_cf_lists.append(row)
        if (last is None) or (x != last):
            sam.extend_run(x); last = x
    return y_cf_lists

def _lcg_worker_index_level_sam_btm(args):
    """
    批量 Index‑LCG：保持签名不变；优先用 Numba 内核。
    入:
      - T: int
      - K: int
      - z_btm_np: [B,T,M] int32
      - cand_bmtk_np: [B,M,T,topk] int32
      - pos_mask_bt: Optional [B,T] bool
    出:
      - y_cf_bmtk: [B,M,T,topk] int16 ∈ {-2, -1, 0..K-1}
    """
    (T, K, z_btm_np, cand_bmtk_np, pos_mask_bt) = args
    if _NUMBA_OK:
        if pos_mask_bt is None:
            mask = _np.ones((z_btm_np.shape[0], z_btm_np.shape[1]), dtype=_np.bool_)
        else:
            mask = pos_mask_bt.astype(_np.bool_)
        y_cf = _lcg_index_level_bmt_nb(z_btm_np, int(K), cand_bmtk_np, mask)
        return y_cf
    # 回退：逐 (b,m) 循环
    B, _, M = z_btm_np.shape
    topk = cand_bmtk_np.shape[-1]
    y_cf = _np.empty((B, M, T, topk), dtype=_np.int16)
    for b in range(B):
        pos_mask = None if pos_mask_bt is None else [bool(x) for x in list(pos_mask_bt[b])]
        for m in range(M):
            z_seq = z_btm_np[b, :, m].tolist()
            cand_lists = cand_bmtk_np[b, m].tolist()
            y_lists = _lcg_route_worker_index_level_sam((T, K, z_seq, cand_lists, pos_mask))
            y_cf[b, m] = _np.asarray(y_lists, dtype=_np.int16)
    return y_cf



# ======================================================
# LCG：自定义 Autograd Function（把对 logits 的梯度“注入”，v 的梯度原路返回）
# ======================================================
class MultiRouteLCGFunction(torch.autograd.Function):
    @staticmethod
    def forward(ctx,
                v_in: torch.Tensor,                 # [B,T,d]
                logits_all: torch.Tensor,           # [B,T,M,K]
                y_idx: torch.Tensor,                # [B,T,M]，0/命中+1
                E_stack: torch.Tensor,              # [M,K+1,d] (detach)
                z_host_pinned: torch.Tensor,        # [B,T,M] int32, CPU pinned
                pos_mask_cpu: list | None):         # [B][T] 或 None
        # 保存给 backward
        ctx.save_for_backward(logits_all, y_idx, E_stack)
        ctx.z_host_pinned = z_host_pinned           # pinned CPU tensor
        ctx.pos_mask_cpu = pos_mask_cpu
        return v_in

    @staticmethod
    def backward(ctx, grad_v: torch.Tensor):
        (logits_all, y_idx, E_stack) = ctx.saved_tensors
        device = logits_all.device
        B, T, M, K = logits_all.shape

        # ---- 1) 概率与 Top-k 候选（在 backward 内一次性计算） ----
        with torch.no_grad():
            p_all = torch.softmax(logits_all.float(), dim=-1)               # [B,T,M,K]
            p_all_bmtk = p_all.permute(0, 2, 1, 3).contiguous()             # [B,M,T,K]
            topk = min(LCG_TOPK, K)
            idx_topk_bmt = torch.topk(p_all_bmtk, k=topk, dim=-1).indices   # [B,M,T,topk]

        # ---- 2) 异步拷贝候选到 pinned CPU，并在后台线程里等待事件后跑 Numba LCG ----
        cand_host = torch.empty_like(idx_topk_bmt, device='cpu', pin_memory=True)
        cand_host.copy_(idx_topk_bmt, non_blocking=True)
        ev = torch.cuda.Event(); ev.record(torch.cuda.current_stream())

        # 准备 z / mask 的 numpy 视图（均在 CPU）
        z_btm_np = np.asarray(ctx.z_host_pinned, order="C")                  # [B,T,M] int32

        # —— 关键修复：确保传给 Numba 的掩码不是 None，而是 2D bool 数组 [B,T] ——
        if ctx.pos_mask_cpu is None:
            pos_mask_bt_np = np.ones((B, T), dtype=np.bool_)                 # 全 True
        else:
            # list[list[bool]] -> np.bool_ 的 C 连续数组
            pos_mask_bt_np = np.asarray(ctx.pos_mask_cpu, dtype=np.bool_, order="C")
            # 兜底检查形状
            if pos_mask_bt_np.ndim != 2 or pos_mask_bt_np.shape != (B, T):
                pos_mask_bt_np = np.ones((B, T), dtype=np.bool_)

        def _wait_and_lcg():
            _wait_event(ev)
            cand_np = np.asarray(cand_host, order="C")                       # [B,M,T,topk] int32
            if _NUMBA_OK and ('_lcg_index_level_bmt_nb' in globals()):
                # 这里始终传 2D bool 掩码，避免 getitem(None, i) 的 TypingError
                return _lcg_index_level_bmt_nb(z_btm_np, int(K), cand_np, pos_mask_bt_np)
            else:
                # 回退实现也传非 None 掩码，接口一致
                return _lcg_worker_index_level_sam_btm((T, int(K), z_btm_np, cand_np, pos_mask_bt_np))

        fut_cf = _ROSA_THREAD_POOL.submit(_wait_and_lcg)

        # ---- 3) GPU 侧可提前计算的部分：S = g · E^T ----
        with torch.no_grad():
            g_no_scale = grad_v.float()                                       # [B,T,d]
            E_stack_f = E_stack.float()                                       # [M,K+1,d]
            # S[b,m,t,k] = g[b,t,:] · E[m,k,:]
            S = torch.einsum("btd,mkd->bmtk", g_no_scale, E_stack_f).to(device)  # [B,M,T,K+1]

            # 旧 y 索引，扩到 topk 形状
            y_old_bmt = y_idx.permute(0, 2, 1)                                     # [B,M,T] in [0..K]
            y_old_exp = y_old_bmt.unsqueeze(-1).expand(-1, -1, -1, topk)           # [B,M,T,topk]

        # ---- 4) 取回 LCG 反事实结果，组装 ΔL / 梯度 ----
        y_cf_bmtk_np = fut_cf.result()                                         # [B,M,T,topk] int16 ∈ {-2,-1,0..K-1}
        with torch.no_grad():
            y_cf_bmtk = torch.from_numpy(y_cf_bmtk_np).to(device=device, dtype=torch.long)
            skip_mask = (y_cf_bmtk == -2)
            pad_mask  = (y_cf_bmtk == -1)
            idx_new   = torch.where(pad_mask, torch.zeros_like(y_cf_bmtk), y_cf_bmtk + 1)  # [B,M,T,topk] ∈ [0..K]

            # ΔL = (g·E_new) - (g·E_old)；跳过位置置 0
            S_new = torch.gather(S, dim=-1, index=idx_new)                          # [B,M,T,topk]
            S_old = torch.gather(S, dim=-1, index=y_old_exp)                         # [B,M,T,topk]
            deltas = S_new - S_old
            deltas = torch.where(skip_mask, torch.zeros_like(deltas), deltas)        # [B,M,T,topk]

            # 概率按 [B,M,T,topk] 对齐
            probs_topk = torch.gather(p_all_bmtk, dim=-1, index=idx_topk_bmt)        # [B,M,T,topk]

            # 对比式梯度（无温度）
            mean_delta = (probs_topk * deltas).sum(dim=-1, keepdim=True)             # [B,M,T,1]
            grad_topk  = probs_topk * (deltas - mean_delta)                           # [B,M,T,topk]

            # scatter_add 回到 logits_all 形状
            grad_logits = torch.zeros_like(logits_all, dtype=p_all.dtype, device=device)     # [B,T,M,K]
            grad_logits_flat = grad_logits.permute(0, 2, 1, 3).contiguous().view(B * M * T, K)
            idx_flat = idx_topk_bmt.contiguous().view(B * M * T, topk)
            src_flat = grad_topk.contiguous().view(B * M * T, topk)
            grad_logits_flat.scatter_add_(dim=1, index=idx_flat, src=src_flat)
            grad_logits = grad_logits_flat.view(B, M, T, K).permute(0, 2, 1, 3).to(logits_all.dtype)

        # 返回各输入的梯度（v 的梯度原样通过；其余不需要梯度返回 None）
        return grad_v, grad_logits, None, None, None, None






# ======================================================
# Qwen3 打补丁：添加多路 ROSA + LCG
# ======================================================
import os
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional, Tuple, List
from transformers.models.qwen3.modeling_qwen3 import Qwen3ForCausalLM, Qwen3DecoderLayer

def patch_qwen3_with_multiroute_rosa(model: Qwen3ForCausalLM):
    base_param = model.model.embed_tokens.weight
    base_dtype = base_param.dtype
    base_device = base_param.device

    hidden_size = model.config.hidden_size
    L = model.config.num_hidden_layers
    M = int(ROSA_NUM_ROUTES)
    K = int(ROSA_VOCAB_SIZE)

    query_impl_env = os.environ.get("ROSA_QUERY_IMPL", "sam_cpu").lower()  # 'sam_cpu' | 'sam_gpu'
    lcg_enable = os.environ.get("LCG_ENABLE", "0") in ("1", "true", "True")


    for li, layer in enumerate(model.model.layers):
        if li == 0:
            continue  # 第 0 层保留全局注意力

        # M 路 logits 头 + M 路 Embedding（E[0]=0）
        layer.rosa_wlm_list = nn.ModuleList(
            [nn.Linear(hidden_size, K, bias=False).to(dtype=base_dtype, device=base_device) for _ in range(M)]
        )
        for w in layer.rosa_wlm_list:
            nn.init.xavier_uniform_(w.weight)

        layer.rosa_emb_list = nn.ModuleList(
            [nn.Embedding(K + 1, hidden_size).to(dtype=base_dtype, device=base_device) for _ in range(M)]
        )
        for emb in layer.rosa_emb_list:
            nn.init.normal_((emb.weight), mean=0.0, std=0.02)
            with torch.no_grad():
                emb.weight.data[0].zero_()

        layer.rosa_num_routes = M
        layer.rosa_vocab_size = K

        def _forward_with_multiroute_rosa(self: Qwen3DecoderLayer,
                                        hidden_states: torch.Tensor,
                                        attention_mask: Optional[torch.Tensor] = None,
                                        position_ids: Optional[torch.LongTensor] = None,
                                        past_key_values=None,
                                        use_cache: Optional[bool] = False,
                                        cache_position: Optional[torch.LongTensor] = None,
                                        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
                                        **kwargs):
            residual = hidden_states
            u = self.input_layernorm(hidden_states)  # [B,T,H]
            B, T, H = u.shape
            M = self.rosa_num_routes
            K = self.rosa_vocab_size

            # ===== 1) 各路 logits & z（dtype 保持一致，避免无谓 cast）=====
            logits_list, z_list_gpu = [], []
            for head in self.rosa_wlm_list:
                if u.dtype is not head.weight.dtype:
                    logits_m = F.linear(u.to(head.weight.dtype), head.weight, None).to(u.dtype)
                else:
                    logits_m = F.linear(u, head.weight, None)  # [B,T,K]
                logits_list.append(logits_m)
                z_list_gpu.append(torch.argmax(logits_m, dim=-1))  # [B,T]
            logits_all = torch.stack(logits_list, dim=2)  # [B,T,M,K]
            z_gpu = torch.stack(z_list_gpu, dim=2)        # [B,T,M]

            # ===== 2) 异步：z -> pinned CPU，并在线程里等事件后跑 ROSA =====
            nvtx.range_push("ROSA_async_copy_and_run")
            z_i32 = z_gpu.to(torch.int32)
            z_host = torch.empty_like(z_i32, device='cpu', pin_memory=True)  # pinned
            z_host.copy_(z_i32, non_blocking=True)
            ev = torch.cuda.Event(); ev.record(torch.cuda.current_stream())

            def _wait_and_rosa():
                _wait_event(ev)
                z_np = np.asarray(z_host, order="C")
                if _NUMBA_OK and ('_rosa_batch_btm_nb' in globals()):
                    return _rosa_batch_btm_nb(z_np, int(K))               # [B,T,M] int32 in {-1,0..K-1}
                else:
                    return _rosa_worker_batch_btm_sam_exact(z_np, int(K)) # 兼容回退
            fut_y = _ROSA_THREAD_POOL.submit(_wait_and_rosa)
            nvtx.range_pop()

            # ===== 3) 与 ROSA 并行：窗口注意力 =====
            nvtx.range_push("attn")
            attn_out, _ = self.self_attn(
                hidden_states=u, attention_mask=attention_mask,
                position_ids=position_ids, past_key_values=past_key_values,
                use_cache=use_cache, cache_position=cache_position,
                position_embeddings=position_embeddings, **kwargs,
            )
            nvtx.range_pop()

            # ===== 4) 取回 y，并查表平均注入 =====
            nvtx.range_push("ROSA_gather_embed")
            y_np  = fut_y.result()  # [B,T,M] int32
            y_btm = torch.from_numpy(y_np).to(device=u.device, dtype=torch.long, non_blocking=True)
            y_idx = torch.where(y_btm >= 0, y_btm + 1, torch.zeros_like(y_btm))  # 0=pad，>0=命中+1

            e_sum = 0
            E_stack_gpu = []
            for m, emb in enumerate(self.rosa_emb_list):
                e_m = F.embedding(y_idx[:, :, m], emb.weight)  # [B,T,d]
                e_sum = e_sum + e_m
                E_stack_gpu.append(emb.weight.detach())
            v = (e_sum / float(M)).to(u.dtype)
            E_stack_gpu = torch.stack(E_stack_gpu, dim=0)      # [M,K+1,d]
            nvtx.range_pop()

            # ===== 5) 可选 LCG（保持逻辑；topk 挪到 backward）=====
            if os.environ.get("LCG_ENABLE", "0") in ("1", "true", "True"):
                pos_mask_cpu = None
                if LCG_POS_SUBSAMPLE < 1.0:
                    mask = (torch.rand((B, T), device=logits_all.device) < LCG_POS_SUBSAMPLE)
                    pos_mask_cpu = mask.cpu().tolist()
                # 把 pinned 的 z_host 直接传入 Autograd ctx（供 backward 的 LCG 使用）
                v = MultiRouteLCGFunction.apply(v, logits_all, y_idx, E_stack_gpu, z_host, pos_mask_cpu)

            # ===== 6) 残差 & MLP（不变）=====
            hidden_states = residual + attn_out + v
            residual = hidden_states
            hidden_states = self.post_attention_layernorm(hidden_states)
            hidden_states = self.mlp(hidden_states)
            hidden_states = residual + hidden_states
            return hidden_states


        layer.forward = _forward_with_multiroute_rosa.__get__(layer, Qwen3DecoderLayer)



    # 元信息保存（方便检查）
    meta = {
        "apply_layers_from": 1,
        "num_routes_per_layer": ROSA_NUM_ROUTES,
        "vocab_per_route": ROSA_VOCAB_SIZE,
    }
    with open(os.path.join(OUTPUT_DIR, "rosa_meta.json"), "w", encoding="utf-8") as f:
        json.dump(meta, f, ensure_ascii=False, indent=2)


# ======================================================
# 构建模型
# ======================================================
def build_model_and_tokenizer() -> Tuple[Qwen3ForCausalLM, AutoTokenizer]:
    config = AutoConfig.from_pretrained(MODEL_LOCAL_DIR)
    config.sliding_window = ATTN_WINDOW
    config.max_window_layers = FIRST_GLOBAL_LAYERS  # 前若干层使用 full attention
    if (not hasattr(config, "layer_types")) or (config.layer_types is None):
        config.layer_types = [
            "full_attention" if i < config.max_window_layers else "sliding_attention"
            for i in range(config.num_hidden_layers)
        ]
    if hasattr(config, "attn_implementation"):
        config.attn_implementation = "flash_attention_2" if USE_FLASH_ATTN else "sdpa"
    else:
        config._attn_implementation = "flash_attention_2" if USE_FLASH_ATTN else "sdpa"

    tokenizer = AutoTokenizer.from_pretrained(MODEL_LOCAL_DIR, use_fast=True)
    model = Qwen3ForCausalLM.from_pretrained(
        MODEL_LOCAL_DIR,
        config=config,
        torch_dtype=torch.bfloat16 if BF16 else torch.float16,
        low_cpu_mem_usage=True,
    )

    if GRADIENT_CHECKPOINTING:
        model.gradient_checkpointing_enable()
    else:
        model.gradient_checkpointing_disable()

    model.config.use_cache = False

    patch_qwen3_with_multiroute_rosa(model)
    return model, tokenizer


# ======================================================
# 仅保存新增参数（多路）
# ======================================================
def save_rosa_only(model: Qwen3ForCausalLM, out_dir: str):
    state = {}
    for i, layer in enumerate(model.model.layers):
        if hasattr(layer, "rosa_wlm_list"):
            for m, head in enumerate(layer.rosa_wlm_list):
                state[f"model.layers.{i}.rosa_wlm_list.{m}.weight"] = head.weight.detach().cpu()
        if hasattr(layer, "rosa_emb_list"):
            for m, emb in enumerate(layer.rosa_emb_list):
                state[f"model.layers.{i}.rosa_emb_list.{m}.weight"] = emb.weight.detach().cpu()
    path = os.path.join(out_dir, SAVE_STATE_DICT_NAME)
    torch.save(state, path)
    print(f"[save] saved ROSA-only params to: {path}")


# ======================================================
# 优化器 param groups
# ======================================================
def build_optimizer_params(model):
    wlm_params, emb_params, backbone_params = [], [], []
    for n, p in model.named_parameters():
        if "rosa_wlm_list" in n:
            wlm_params.append(p)
        elif "rosa_emb_list" in n:
            emb_params.append(p)
        else:
            backbone_params.append(p)

    param_groups = [
        {"params": wlm_params, "lr": LR_ROSA, "weight_decay": WEIGHT_DECAY},
        {"params": emb_params, "lr": LR_ROSA, "weight_decay": 0.0},
    ]
    if LR_BACKBONE and LR_BACKBONE > 0.0:
        no_decay, has_decay = [], []
        for n, p in model.named_parameters():
            if ("rosa_" in n):
                continue
            if any(k in n.lower() for k in ["bias", "norm", "layernorm", "ln"]):
                no_decay.append(p)
            else:
                has_decay.append(p)
        param_groups += [
            {"params": has_decay, "lr": LR_BACKBONE, "weight_decay": WEIGHT_DECAY},
            {"params": no_decay, "lr": LR_BACKBONE, "weight_decay": 0.0},
        ]
    else:
        for p in backbone_params:
            p.requires_grad_(False)

    return param_groups


# ======================================================
# 回调：确保 E[0]=0
# ======================================================
from transformers.trainer_callback import TrainerCallback

def is_main_process() -> bool:
    return _env_int("RANK", 0) == 0

class RosaZeroRowCallback(TrainerCallback):
    """确保每层每路 E[0] 始终为零"""
    def on_init_end(self, args, state, control, model=None, **kwargs):
        if model is not None:
            with torch.no_grad():
                for layer in model.model.layers:
                    if hasattr(layer, "rosa_emb_list"):
                        for emb in layer.rosa_emb_list:
                            emb.weight.data[0].zero_()
        return control

    def on_step_end(self, args, state, control, model=None, **kwargs):
        if model is not None:
            with torch.no_grad():
                for layer in model.model.layers:
                    if hasattr(layer, "rosa_emb_list"):
                        for emb in layer.rosa_emb_list:
                            emb.weight.data[0].zero_()
        return control


# ======================================================
# 训练主逻辑
# ======================================================
def main():
    set_seed(SEED)

    raw = load_from_disk(DATASET_DIR)
    train_ds = raw["train"]
    test_ds = raw.get("test", raw["validation"] if "validation" in raw else None)
    assert test_ds is not None, "需要存在 test 或 validation split"

    model, tokenizer = build_model_and_tokenizer()
    pad_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id
    data_collator = FixedLenLMCollator(pad_token_id=pad_id, seq_len=SEQ_LEN)

    training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        num_train_epochs=NUM_EPOCHS,
        per_device_train_batch_size=PER_DEVICE_TRAIN_BSZ,
        per_device_eval_batch_size=1,
        gradient_accumulation_steps=GRAD_ACCUM_STEPS,
        learning_rate=LR_ROSA,
        warmup_steps=WARMUP_STEPS,
        logging_steps=LOGGING_STEPS,
        eval_steps=EVAL_STEPS,
        save_strategy="no",
        report_to="none",
        fp16=(not BF16) and torch.cuda.is_available(),
        bf16=BF16,
        dataloader_num_workers=2,
        gradient_checkpointing=GRADIENT_CHECKPOINTING,
        remove_unused_columns=False,
        optim="adamw_torch",
    )

    optimizer_params = build_optimizer_params(model)

    class _Trainer(Trainer):
        def create_optimizer(self):
            if self.optimizer is None:
                self.optimizer = torch.optim.AdamW(optimizer_params, betas=(0.9, 0.98), eps=1e-8)
            return self.optimizer

    trainer = _Trainer(
        model=model,
        args=training_args,
        train_dataset=train_ds,
        eval_dataset=test_ds,
        data_collator=data_collator,
        tokenizer=tokenizer,
        callbacks=[RosaZeroRowCallback()],
    )

    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

    print(f"总参数:        {total_params:,}")

    trainer.train()
    metrics = trainer.evaluate()

    meta = {
        "model_local_dir": MODEL_LOCAL_DIR,
        "dataset_dir": DATASET_DIR,
        "seq_len": SEQ_LEN,
        "attn_window": ATTN_WINDOW,
        "first_global_layers": FIRST_GLOBAL_LAYERS,
        "rosa": {
            "num_routes": ROSA_NUM_ROUTES,
            "vocab_size": ROSA_VOCAB_SIZE,
            "lcg_topk": LCG_TOPK,
            "pos_subsample": LCG_POS_SUBSAMPLE,
            "lr_rosa": LR_ROSA, "lr_backbone": LR_BACKBONE,
            "k_per_layer": {str(i): (getattr(model.model.layers[i], "rosa_vocab_size", 0))
                            for i in range(model.config.num_hidden_layers)}
        },
        "metrics": metrics,
        "time": time.asctime(),
    }

    if is_main_process():
        print("[eval] metrics:", metrics)
        save_rosa_only(model, OUTPUT_DIR)
        with open(os.path.join(OUTPUT_DIR, "run_meta.json"), "w", encoding="utf-8") as f:
            json.dump(meta, f, ensure_ascii=False, indent=2)
        print(f"[done] meta saved at {os.path.join(OUTPUT_DIR, 'run_meta.json')}")


if __name__ == "__main__":
    main()
