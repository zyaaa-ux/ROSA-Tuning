{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5faaf872",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import json\n",
    "import time\n",
    "import atexit\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import multiprocessing as mp\n",
    "\n",
    "from datasets import load_from_disk\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.models.qwen3.modeling_qwen3 import (\n",
    "    Qwen3ForCausalLM,\n",
    "    Qwen3DecoderLayer,\n",
    ")\n",
    "\n",
    "import multiprocessing as mp\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import threading\n",
    "import os\n",
    "import atexit\n",
    "\n",
    "import os as _os\n",
    "_os.environ.setdefault(\"ROSA_USE_NUMBA\", \"\")\n",
    "_os.environ.setdefault(\"ROSA_NUMBA_PARALLEL\", \"\")\n",
    "_os.environ.setdefault(\"LCG_ENABLE\", \"\")\n",
    "\n",
    "_os.environ[\"ROSA_INJECT_MODE\"] = \"\"\n",
    "\n",
    "_USE_NUMBA = _os.environ.get(\"ROSA_USE_NUMBA\", \"\").lower() not in (\"0\", \"false\")\n",
    "_PARALLEL  = _os.environ.get(\"ROSA_NUMBA_PARALLEL\", \"\").lower() not in (\"0\", \"false\")\n",
    "\n",
    "try:\n",
    "    import numba as _nb\n",
    "    _NUMBA_OK = _USE_NUMBA\n",
    "except Exception:\n",
    "    _NUMBA_OK = False\n",
    "\n",
    "\n",
    "MODEL_LOCAL_DIR = \"\"\n",
    "DATASET_DIR     = \"\"\n",
    "OUTPUT_DIR      = \"\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "USE_FLASH_ATTN = None\n",
    "BF16 = None\n",
    "\n",
    "SEQ_LEN = None\n",
    "ATTN_WINDOW = None\n",
    "FIRST_GLOBAL_LAYERS = None\n",
    "\n",
    "ROSA_NUM_ROUTES: int = None\n",
    "ROSA_VOCAB_SIZE: int = None\n",
    "LCG_TOPK: int = None\n",
    "LCG_POS_SUBSAMPLE: float = None\n",
    "ROSA_CPU_WORKERS: int = None\n",
    "\n",
    "\n",
    "LR_ROSA = None\n",
    "LR_BACKBONE = None\n",
    "WEIGHT_DECAY = None\n",
    "WARMUP_STEPS = None\n",
    "NUM_EPOCHS = None\n",
    "PER_DEVICE_TRAIN_BSZ = None\n",
    "GRAD_ACCUM_STEPS = None\n",
    "LOGGING_STEPS = None\n",
    "EVAL_STEPS = None\n",
    "SEED = None\n",
    "\n",
    "SAVE_STATE_DICT_NAME = \"\"\n",
    "\n",
    "GRADIENT_CHECKPOINTING = None\n",
    "\n",
    "_ROSA_POOL = None\n",
    "_ROSA_POOL_PID = None\n",
    "_ROSA_POOL_LOCK = threading.Lock()\n",
    "\n",
    "\n",
    "def _env_int(k, default):\n",
    "    try:\n",
    "        return int(os.environ.get(k, default))\n",
    "    except Exception:\n",
    "        return default\n",
    "\n",
    "LOCAL_WORLD_SIZE = _env_int(\"LOCAL_WORLD_SIZE\", max(1, torch.cuda.device_count()))\n",
    "LOCAL_RANK = _env_int(\"LOCAL_RANK\", 0)\n",
    "GLOBAL_RANK = _env_int(\"RANK\", 0)\n",
    "\n",
    "try:\n",
    "    import torch.cuda.nvtx as nvtx\n",
    "except Exception:\n",
    "    class _DummyNVTX:\n",
    "        def range_push(self, *a, **k): pass\n",
    "        def range_pop(self): pass\n",
    "    nvtx = _DummyNVTX()\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "_ROSA_THREAD_POOL = ThreadPoolExecutor(\n",
    "    max_workers=max(1, _env_int(\"ROSA_THREAD_WORKERS\", 2))\n",
    ")\n",
    "\n",
    "def _wait_event(ev: \"torch.cuda.Event\"):\n",
    "    try:\n",
    "        ev.synchronize()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "def _available_logical_cpus() -> int:\n",
    "    try:\n",
    "        if hasattr(os, \"sched_getaffinity\"):\n",
    "            return max(1, len(os.sched_getaffinity(0)))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    def _parse_cpuset(s: str) -> int:\n",
    "        n = 0\n",
    "        for part in s.split(\",\"):\n",
    "            part = part.strip()\n",
    "            if not part:\n",
    "                continue\n",
    "            if \"-\" in part:\n",
    "                a, b = part.split(\"-\")\n",
    "                n += int(b) - int(a) + 1\n",
    "            else:\n",
    "                n += 1\n",
    "        return n\n",
    "\n",
    "    try:\n",
    "        for p in (\"/sys/fs/cgroup/cpuset.cpus\", \"/sys/fs/cgroup/cpuset/cpuset.cpus\"):\n",
    "            if os.path.exists(p):\n",
    "                with open(p) as f:\n",
    "                    txt = f.read().strip()\n",
    "                if txt:\n",
    "                    return max(1, _parse_cpuset(txt))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return max(1, os.cpu_count() or 1)\n",
    "\n",
    "_TOTAL_CPUS = _available_logical_cpus()\n",
    "_PER_RANK_CPUS = max(1, _TOTAL_CPUS // max(1, LOCAL_WORLD_SIZE))\n",
    "\n",
    "\n",
    "torch.set_num_threads(_PER_RANK_CPUS)\n",
    "\n",
    "\n",
    "try:\n",
    "    import rosa_cpp\n",
    "    _ROSA_CPP_OK = True\n",
    "except Exception:\n",
    "    _ROSA_CPP_OK = False\n",
    "\n",
    "def _rosa_lcg_prefix_ext(z_host_cpu_i32: torch.Tensor,\n",
    "                         cand_host_cpu_i32: torch.Tensor,\n",
    "                         pos_mask_cpu_bool: Optional[torch.Tensor],\n",
    "                         K: int):\n",
    "    assert z_host_cpu_i32.device.type == \"cpu\" and z_host_cpu_i32.dtype == torch.int32\n",
    "    assert cand_host_cpu_i32.device.type == \"cpu\" and cand_host_cpu_i32.dtype == torch.int32\n",
    "    if pos_mask_cpu_bool is not None:\n",
    "        assert pos_mask_cpu_bool.device.type == \"cpu\" and pos_mask_cpu_bool.dtype == torch.bool\n",
    "    if not _ROSA_CPP_OK:\n",
    "        raise RuntimeError(\"rosa_cpp extension not available; build it first.\")\n",
    "    return rosa_cpp.rosa_lcg_prefix(z_host_cpu_i32.contiguous(),\n",
    "                                    cand_host_cpu_i32.contiguous(),\n",
    "                                    pos_mask_cpu_bool, int(K))\n",
    "\n",
    "\n",
    "def _rosa_worker_init():\n",
    "    os.environ.setdefault(\"OMP_NUM_THREADS\", \"\")\n",
    "    os.environ.setdefault(\"MKL_NUM_THREADS\", \"\")\n",
    "    try:\n",
    "        import torch as _t\n",
    "        _t.set_num_threads(1)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "import numpy as _np\n",
    "import torch\n",
    "\n",
    "def to_c_np(t: torch.Tensor, dtype=_np.int32):\n",
    "    return _np.asarray(\n",
    "        t.detach().to(torch.int32 if dtype is _np.int32 else torch.bool).contiguous().cpu().numpy(),\n",
    "        order=\"C\",\n",
    "        dtype=dtype,\n",
    "    )\n",
    "\n",
    "def to_c_bool_np(t: torch.Tensor):\n",
    "    return _np.asarray(t.detach().to(torch.bool).contiguous().cpu().numpy(), order=\"C\", dtype=_np.bool_)\n",
    "\n",
    "try:\n",
    "    import rosa_cpp\n",
    "    _ROSA_CPP_OK = True\n",
    "except Exception:\n",
    "    _ROSA_CPP_OK = False\n",
    "\n",
    "def _rosa_batch_btm_with_ws_ext(z_host_cpu_i32: torch.Tensor, K: int):\n",
    "    assert z_host_cpu_i32.device.type == \"cpu\" and z_host_cpu_i32.dtype == torch.int32\n",
    "    if not _ROSA_CPP_OK:\n",
    "        raise RuntimeError(\"rosa_cpp extension not available; please build it or fallback to numba path.\")\n",
    "    return rosa_cpp.rosa_batch_btm_with_ws(z_host_cpu_i32.contiguous(), int(K))\n",
    "\n",
    "\n",
    "class _SAMFoldedCPU:\n",
    "    __slots__ = (\"next\", \"link\", \"length\", \"e\", \"last\", \"size\", \"K\", \"c\", \"last_sym\")\n",
    "\n",
    "    def __init__(self, max_states: int, K: int):\n",
    "        self.K = int(K)\n",
    "        S = int(max_states)\n",
    "        self.next   = _np.full((S, self.K), -1, dtype=_np.int32)\n",
    "        self.link   = _np.full((S,),       -1, dtype=_np.int32)\n",
    "        self.length = _np.zeros((S,),          dtype=_np.int32)\n",
    "        self.e      = _np.full((S,),       -1, dtype=_np.int32)\n",
    "        self.last   = 0\n",
    "        self.size   = 1\n",
    "        self.c: List[int] = []\n",
    "        self.last_sym: Optional[int] = None\n",
    "\n",
    "    def _new_state(self, L: int) -> int:\n",
    "        s = self.size\n",
    "        self.size += 1\n",
    "        self.length[s] = L\n",
    "        self.link[s] = -1\n",
    "        self.e[s] = -1\n",
    "        self.next[s, :].fill(-1)\n",
    "        return s\n",
    "\n",
    "    def match_next(self, x: int) -> int:\n",
    "        p = self.last\n",
    "        nxt = self.next\n",
    "        link = self.link\n",
    "        while p != -1 and nxt[p, x] == -1:\n",
    "            p = link[p]\n",
    "        return -1 if p == -1 else int(nxt[p, x])\n",
    "\n",
    "    def nextdiff_from_state(self, q: int) -> int:\n",
    "        if q == -1:\n",
    "            return -1\n",
    "        rpos = int(self.e[q])\n",
    "        nxt = rpos + 1\n",
    "        if 0 <= rpos and nxt < len(self.c):\n",
    "            return int(self.c[nxt])\n",
    "        return -1\n",
    "\n",
    "    def extend_run(self, x: int):\n",
    "        self.c.append(x)\n",
    "        pos = len(self.c) - 1\n",
    "\n",
    "        last = self.last\n",
    "        nxt = self.next\n",
    "        link = self.link\n",
    "        length = self.length\n",
    "        e = self.e\n",
    "\n",
    "        cur = self._new_state(length[last] + 1)\n",
    "        p = last\n",
    "        while p != -1 and nxt[p, x] == -1:\n",
    "            nxt[p, x] = cur\n",
    "            p = link[p]\n",
    "        if p == -1:\n",
    "            link[cur] = 0\n",
    "        else:\n",
    "            q = int(nxt[p, x])\n",
    "            if length[p] + 1 == length[q]:\n",
    "                link[cur] = q\n",
    "            else:\n",
    "                clone = self._new_state(length[p] + 1)\n",
    "                nxt[clone, :] = nxt[q, :]\n",
    "                link[clone]   = link[q]\n",
    "                e[clone]      = e[q]\n",
    "                while p != -1 and nxt[p, x] == q:\n",
    "                    nxt[p, x] = clone\n",
    "                    p = link[p]\n",
    "                link[q]   = clone\n",
    "                link[cur] = clone\n",
    "\n",
    "        v = cur\n",
    "        while v != -1 and e[v] != pos:\n",
    "            e[v] = pos\n",
    "            v = link[v]\n",
    "\n",
    "        self.last = cur\n",
    "        self.last_sym = x\n",
    "\n",
    "    def query_then_commit(self, x: int) -> int:\n",
    "        q = self.match_next(x)\n",
    "        a = self.nextdiff_from_state(q)\n",
    "        if self.last_sym is None or x != self.last_sym:\n",
    "            self.extend_run(x)\n",
    "        return a\n",
    "\n",
    "\n",
    "class _PinnedBufferPool:\n",
    "    def __init__(self):\n",
    "        self._pool = {}\n",
    "\n",
    "    def get(self, tag: str, shape: tuple, dtype: torch.dtype = torch.int32):\n",
    "        key = (tag, shape, dtype)\n",
    "        t = self._pool.get(key, None)\n",
    "        if t is None or t.shape != torch.Size(shape) or t.dtype != dtype:\n",
    "            t = torch.empty(shape, dtype=dtype, device='cpu', pin_memory=True)\n",
    "            self._pool[key] = t\n",
    "        return t\n",
    "\n",
    "_PINNED_POOL = _PinnedBufferPool()\n",
    "\n",
    "\n",
    "\n",
    "if _NUMBA_OK:\n",
    "    @(_nb.njit(cache=True, fastmath=False))\n",
    "    def _count_runs_nb(arr):\n",
    "        T = arr.shape[0]\n",
    "        if T == 0: \n",
    "            return 0\n",
    "        cnt = 1\n",
    "        last = arr[0]\n",
    "        for i in range(1, T):\n",
    "            if arr[i] != last:\n",
    "                cnt += 1\n",
    "                last = arr[i]\n",
    "        return cnt\n",
    "\n",
    "    @(_nb.njit(cache=True, fastmath=False, inline='always'))\n",
    "    def _sam_new_state(next_arr, link, length, e, size, L):\n",
    "        s = size[0]\n",
    "        size[0] = s + 1\n",
    "        length[s] = L\n",
    "        link[s]   = -1\n",
    "        e[s]      = -1\n",
    "        for j in range(next_arr.shape[1]):\n",
    "            next_arr[s, j] = -1\n",
    "        return s\n",
    "\n",
    "    @(_nb.njit(cache=True, fastmath=False))\n",
    "    def _sam_extend(next_arr, link, length, e, last_arr, size, x, pos):\n",
    "        last = last_arr[0]\n",
    "        cur = _sam_new_state(next_arr, link, length, e, size, length[last] + 1)\n",
    "        p = last\n",
    "        K = next_arr.shape[1]\n",
    "\n",
    "        while p != -1 and next_arr[p, x] == -1:\n",
    "            next_arr[p, x] = cur\n",
    "            p = link[p]\n",
    "\n",
    "        if p == -1:\n",
    "            link[cur] = 0\n",
    "        else:\n",
    "            q = next_arr[p, x]\n",
    "            if length[p] + 1 == length[q]:\n",
    "                link[cur] = q\n",
    "            else:\n",
    "                clone = _sam_new_state(next_arr, link, length, e, size, length[p] + 1)\n",
    "                for j in range(K):\n",
    "                    next_arr[clone, j] = next_arr[q, j]\n",
    "                link[clone] = link[q]\n",
    "                e[clone]    = e[q]\n",
    "                while p != -1 and next_arr[p, x] == q:\n",
    "                    next_arr[p, x] = clone\n",
    "                    p = link[p]\n",
    "                link[q]   = clone\n",
    "                link[cur] = clone\n",
    "\n",
    "        v = cur\n",
    "        while v != -1 and e[v] != pos:\n",
    "            e[v] = pos\n",
    "            v = link[v]\n",
    "\n",
    "        last_arr[0] = cur\n",
    "\n",
    "    @(_nb.njit(cache=True, fastmath=False, inline='always'))\n",
    "    def _sam_match_next(next_arr, link, last_state, x):\n",
    "        p = last_state\n",
    "        while p != -1 and next_arr[p, x] == -1:\n",
    "            p = link[p]\n",
    "        if p == -1:\n",
    "            return -1\n",
    "        return next_arr[p, x]\n",
    "\n",
    "    @(_nb.njit(cache=True, fastmath=False))\n",
    "    def _rosa_seq_folded_nb(z, K):\n",
    "        T = z.shape[0]\n",
    "        y = _np.empty((T,), dtype=_np.int32)\n",
    "        if T == 0:\n",
    "            return y\n",
    "\n",
    "        R = _count_runs_nb(z)\n",
    "        S_cap = 2 * R + 5\n",
    "        next_arr = _np.empty((S_cap, K), dtype=_np.int32)\n",
    "        link     = _np.empty((S_cap,),   dtype=_np.int32)\n",
    "        length   = _np.empty((S_cap,),   dtype=_np.int32)\n",
    "        e        = _np.empty((S_cap,),   dtype=_np.int32)\n",
    "        for j in range(K):\n",
    "            next_arr[0, j] = -1\n",
    "        link[0]   = -1\n",
    "        length[0] = 0\n",
    "        e[0]      = -1\n",
    "        size = _np.empty((1,), dtype=_np.int32)\n",
    "        size[0] = 1\n",
    "        last_arr = _np.empty((1,), dtype=_np.int32)\n",
    "        last_arr[0] = 0\n",
    "\n",
    "        c = _np.empty((T,), dtype=_np.int32)\n",
    "        c_len = 0\n",
    "        last_sym = -2147483647\n",
    "\n",
    "        for t in range(T):\n",
    "            x = z[t]\n",
    "            q = _sam_match_next(next_arr, link, last_arr[0], x)\n",
    "            if q == -1:\n",
    "                y[t] = -1\n",
    "            else:\n",
    "                rpos = e[q]\n",
    "                nxt  = rpos + 1\n",
    "                if 0 <= rpos and nxt < c_len:\n",
    "                    y[t] = c[nxt]\n",
    "                else:\n",
    "                    y[t] = -1\n",
    "            if t == 0 or x != last_sym:\n",
    "                c[c_len] = x\n",
    "                c_len += 1\n",
    "                _sam_extend(next_arr, link, length, e, last_arr, size, x, c_len - 1)\n",
    "                last_sym = x\n",
    "\n",
    "        return y\n",
    "\n",
    "    @(_nb.njit(cache=True, fastmath=False))\n",
    "    def _rosa_batch_btm_nb(z_btm, K):\n",
    "        B, T, M = z_btm.shape\n",
    "        y_btm = _np.empty((B, T, M), dtype=_np.int32)\n",
    "        if _PARALLEL:\n",
    "            for b in _nb.prange(B):\n",
    "                for m in range(M):\n",
    "                    y_btm[b, :, m] = _rosa_seq_folded_nb(z_btm[b, :, m], K)\n",
    "        else:\n",
    "            for b in range(B):\n",
    "                for m in range(M):\n",
    "                    y_btm[b, :, m] = _rosa_seq_folded_nb(z_btm[b, :, m], K)\n",
    "        return y_btm\n",
    "\n",
    "    @(_nb.njit(cache=True, fastmath=False))\n",
    "    def _lcg_index_level_seq_nb(z, K, cand_tm, pos_mask):\n",
    "        T = z.shape[0]\n",
    "        topk = cand_tm.shape[1]\n",
    "        out = _np.empty((T, topk), dtype=_np.int16)\n",
    "        if T == 0:\n",
    "            return out\n",
    "\n",
    "        R = _count_runs_nb(z)\n",
    "        S_cap = 2 * R + 5\n",
    "        next_arr = _np.empty((S_cap, K), dtype=_np.int32)\n",
    "        link     = _np.empty((S_cap,),   dtype=_np.int32)\n",
    "        length   = _np.empty((S_cap,),   dtype=_np.int32)\n",
    "        e        = _np.empty((S_cap,),   dtype=_np.int32)\n",
    "        for j in range(K):\n",
    "            next_arr[0, j] = -1\n",
    "        link[0]   = -1\n",
    "        length[0] = 0\n",
    "        e[0]      = -1\n",
    "        size = _np.empty((1,), dtype=_np.int32); size[0] = 1\n",
    "        last_arr = _np.empty((1,), dtype=_np.int32); last_arr[0] = 0\n",
    "        c = _np.empty((T,), dtype=_np.int32); c_len = 0\n",
    "        last_sym = -2147483647\n",
    "\n",
    "        for t in range(T):\n",
    "            if not pos_mask[t]:\n",
    "                for j in range(topk):\n",
    "                    out[t, j] = -2\n",
    "            else:\n",
    "                for j in range(topk):\n",
    "                    k = cand_tm[t, j]\n",
    "                    q = _sam_match_next(next_arr, link, last_arr[0], k)\n",
    "                    if q == -1:\n",
    "                        out[t, j] = -1\n",
    "                    else:\n",
    "                        rpos = e[q]; nxt = rpos + 1\n",
    "                        if 0 <= rpos and nxt < c_len:\n",
    "                            out[t, j] = _np.int16(c[nxt])\n",
    "                        else:\n",
    "                            out[t, j] = -1\n",
    "            x = z[t]\n",
    "            if t == 0 or x != last_sym:\n",
    "                c[c_len] = x; c_len += 1\n",
    "                _sam_extend(next_arr, link, length, e, last_arr, size, x, c_len - 1)\n",
    "                last_sym = x\n",
    "\n",
    "        return out\n",
    "\n",
    "    @(_nb.njit(cache=True, fastmath=False))\n",
    "    def _lcg_index_level_bmt_nb(z_btm, K, cand_bmtk, pos_mask_bt):\n",
    "        B, T, M = z_btm.shape\n",
    "        topk = cand_bmtk.shape[3]\n",
    "        out = _np.empty((B, M, T, topk), dtype=_np.int16)\n",
    "        if _PARALLEL:\n",
    "            for b in _nb.prange(B):\n",
    "                for m in range(M):\n",
    "                    out[b, m] = _lcg_index_level_seq_nb(z_btm[b, :, m], K, cand_bmtk[b, m], pos_mask_bt[b])\n",
    "        else:\n",
    "            for b in range(B):\n",
    "                for m in range(M):\n",
    "                    out[b, m] = _lcg_index_level_seq_nb(z_btm[b, :, m], K, cand_bmtk[b, m], pos_mask_bt[b])\n",
    "        return out\n",
    "\n",
    "if _NUMBA_OK:\n",
    "    @(_nb.njit(cache=True, fastmath=False))\n",
    "    def _rosa_seq_with_ws_nb(z, K: int, S_cap: int):\n",
    "        T = z.shape[0]\n",
    "        y = _np.empty((T,), dtype=_np.int32)\n",
    "        last_trace = _np.empty((T,), dtype=_np.int32)\n",
    "\n",
    "        next_arr = _np.empty((S_cap, K), dtype=_np.int32)\n",
    "        link     = _np.empty((S_cap,),   dtype=_np.int32)\n",
    "        length   = _np.empty((S_cap,),   dtype=_np.int32)\n",
    "        e        = _np.empty((S_cap,),   dtype=_np.int32)\n",
    "\n",
    "        for j in range(K): next_arr[0, j] = -1\n",
    "        link[0]   = -1\n",
    "        length[0] = 0\n",
    "        e[0]      = -1\n",
    "        size = _np.empty((1,), dtype=_np.int32); size[0] = 1\n",
    "        last_arr = _np.empty((1,), dtype=_np.int32); last_arr[0] = 0\n",
    "\n",
    "        c = _np.empty((T,), dtype=_np.int32)\n",
    "        c_len = 0\n",
    "        last_sym = -2147483647\n",
    "\n",
    "        for t in range(T):\n",
    "            last_trace[t] = last_arr[0]\n",
    "            x = z[t]\n",
    "            q = _sam_match_next(next_arr, link, last_arr[0], x)\n",
    "            if q == -1:\n",
    "                y[t] = -1\n",
    "            else:\n",
    "                rpos = e[q]; nxt = rpos + 1\n",
    "                if 0 <= rpos and nxt < c_len:\n",
    "                    y[t] = c[nxt]\n",
    "                else:\n",
    "                    y[t] = -1\n",
    "            if t == 0 or x != last_sym:\n",
    "                c[c_len] = x; c_len += 1\n",
    "                _sam_extend(next_arr, link, length, e, last_arr, size, x, c_len - 1)\n",
    "                last_sym = x\n",
    "\n",
    "        return y, last_trace, next_arr, link, e, c, _np.int32(c_len)\n",
    "\n",
    "    @(_nb.njit(cache=True, fastmath=False))\n",
    "    def _rosa_batch_btm_with_ws_nb(z_btm, K: int):\n",
    "        B, T, M = z_btm.shape\n",
    "        S_cap = 2 * T + 5\n",
    "\n",
    "        y_btm         = _np.empty((B, T, M), dtype=_np.int32)\n",
    "        last_btm      = _np.empty((B, T, M), dtype=_np.int32)\n",
    "        next_bmsk     = _np.empty((B, M, S_cap, K), dtype=_np.int32)\n",
    "        link_bms      = _np.empty((B, M, S_cap),     dtype=_np.int32)\n",
    "        e_bms         = _np.empty((B, M, S_cap),     dtype=_np.int32)\n",
    "        c_bmt         = _np.empty((B, M, T),         dtype=_np.int32)\n",
    "        c_len_bm      = _np.empty((B, M),            dtype=_np.int32)\n",
    "\n",
    "        if _PARALLEL:\n",
    "            for idx in _nb.prange(B * M):\n",
    "                b = idx // M\n",
    "                m = idx % M\n",
    "                y, last_tr, nxt, lnk, ee, cc, clen = _rosa_seq_with_ws_nb(z_btm[b, :, m], K, S_cap)\n",
    "                y_btm[b, :, m] = y\n",
    "                last_btm[b, :, m] = last_tr\n",
    "                next_bmsk[b, m, :, :] = nxt\n",
    "                link_bms[b, m, :] = lnk\n",
    "                e_bms[b, m, :] = ee\n",
    "                c_bmt[b, m, :] = 0\n",
    "                for i in range(T):\n",
    "                    if i < clen:\n",
    "                        c_bmt[b, m, i] = cc[i]\n",
    "                c_len_bm[b, m] = clen\n",
    "        else:\n",
    "            for b in range(B):\n",
    "                for m in range(M):\n",
    "                    y, last_tr, nxt, lnk, ee, cc, clen = _rosa_seq_with_ws_nb(z_btm[b, :, m], K, S_cap)\n",
    "                    y_btm[b, :, m] = y\n",
    "                    last_btm[b, :, m] = last_tr\n",
    "                    next_bmsk[b, m, :, :] = nxt\n",
    "                    link_bms[b, m, :] = lnk\n",
    "                    e_bms[b, m, :] = ee\n",
    "                    c_bmt[b, m, :] = 0\n",
    "                    for i in range(T):\n",
    "                        if i < clen:\n",
    "                            c_bmt[b, m, i] = cc[i]\n",
    "                    c_len_bm[b, m] = clen\n",
    "\n",
    "        return y_btm, last_btm, next_bmsk, link_bms, e_bms, c_bmt, c_len_bm\n",
    "\n",
    "def _rosa_batch_btm_with_ws_py(z_btm_np: \"_np.ndarray\", K: int):\n",
    "    B, T, M = z_btm_np.shape\n",
    "    S_cap = 2 * T + 5\n",
    "\n",
    "    y_btm    = _np.empty((B, T, M), dtype=_np.int32)\n",
    "    last_btm = _np.empty((B, T, M), dtype=_np.int32)\n",
    "    next_bmsk = _np.empty((B, M, S_cap, K), dtype=_np.int32)\n",
    "    link_bms  = _np.empty((B, M, S_cap),     dtype=_np.int32)\n",
    "    e_bms     = _np.empty((B, M, S_cap),     dtype=_np.int32)\n",
    "    c_bmt     = _np.empty((B, M, T),         dtype=_np.int32)\n",
    "    c_len_bm  = _np.empty((B, M),            dtype=_np.int32)\n",
    "\n",
    "    for b in range(B):\n",
    "        for m in range(M):\n",
    "            z = [int(v) for v in list(z_btm_np[b, :, m])]\n",
    "            sam = _SAMFoldedCPU(max_states=S_cap, K=K)\n",
    "            y_seq = []\n",
    "            last_trace = []\n",
    "            last_sym = None\n",
    "            for t in range(T):\n",
    "                last_trace.append(int(sam.last))\n",
    "                x = z[t]\n",
    "                q = sam.match_next(int(x))\n",
    "                a = sam.nextdiff_from_state(q)\n",
    "                y_seq.append(-1 if a == -1 else int(a))\n",
    "                if (last_sym is None) or (x != last_sym):\n",
    "                    sam.extend_run(int(x)); last_sym = x\n",
    "\n",
    "            y_btm[b, :, m] = _np.asarray(y_seq, dtype=_np.int32)\n",
    "            last_btm[b, :, m] = _np.asarray(last_trace, dtype=_np.int32)\n",
    "            next_bmsk[b, m, :, :] = sam.next\n",
    "            link_bms[b, m, :] = sam.link\n",
    "            e_bms[b, m, :] = sam.e\n",
    "            c_len = len(sam.c)\n",
    "            c_len_bm[b, m] = c_len\n",
    "            c_bmt[b, m, :] = 0\n",
    "            for i in range(min(T, c_len)):\n",
    "                c_bmt[b, m, i] = sam.c[i]\n",
    "\n",
    "    return y_btm, last_btm, next_bmsk, link_bms, e_bms, c_bmt, c_len_bm\n",
    "\n",
    "\n",
    "def _rosa_batch_btm_with_ws(z_btm_np: \"_np.ndarray\", K: int):\n",
    "    if _NUMBA_OK and ('_rosa_batch_btm_with_ws_nb' in globals()):\n",
    "        return _rosa_batch_btm_with_ws_nb(z_btm_np, int(K))\n",
    "    return _rosa_batch_btm_with_ws_py(z_btm_np, int(K))\n",
    "\n",
    "@torch.no_grad()\n",
    "def _lcg_query_gpu(\n",
    "    next_bmsk: torch.Tensor,\n",
    "    link_bms: torch.Tensor,\n",
    "    e_bms: torch.Tensor,\n",
    "    c_bmt: torch.Tensor,\n",
    "    c_len_bm: torch.Tensor,\n",
    "    last_state_btm: torch.Tensor,\n",
    "    cand_bmtk: torch.Tensor,\n",
    "    pos_mask_bt: Optional[torch.Tensor],\n",
    ") -> torch.Tensor:\n",
    "    device = cand_bmtk.device\n",
    "    B, M, S, K = next_bmsk.shape\n",
    "    T = last_state_btm.size(1)\n",
    "    topk = cand_bmtk.size(-1)\n",
    "\n",
    "    b_idx = torch.arange(B, device=device).view(B, 1, 1, 1).expand(B, M, T, topk).reshape(-1)\n",
    "    m_idx = torch.arange(M, device=device).view(1, M, 1, 1).expand(B, M, T, topk).reshape(-1)\n",
    "    t_idx = torch.arange(T, device=device).view(1, 1, T, 1).expand(B, M, T, topk).reshape(-1)\n",
    "\n",
    "    p = last_state_btm.permute(0, 2, 1).contiguous()\n",
    "    p = p.unsqueeze(-1).expand(-1, -1, -1, topk).reshape(-1).to(torch.long)\n",
    "\n",
    "    x = cand_bmtk.reshape(-1).to(torch.long)\n",
    "\n",
    "    q = torch.full_like(p, -1, dtype=torch.long)\n",
    "    done = (p == -1)\n",
    "\n",
    "    for _ in range(S):\n",
    "        p_clamped = torch.clamp(p, 0, S - 1)\n",
    "        nxt_val = next_bmsk[b_idx, m_idx, p_clamped, x].to(torch.long)\n",
    "        has = (~done) & (nxt_val != -1)\n",
    "        q = torch.where(has, nxt_val, q)\n",
    "        done = done | has | (p == -1)\n",
    "        if bool(done.all()):\n",
    "            break\n",
    "        p_next = link_bms[b_idx, m_idx, p_clamped].to(torch.long)\n",
    "        p = torch.where(~done, p_next, p)\n",
    "\n",
    "    q_valid = (q != -1)\n",
    "    qc = torch.clamp(q, 0, S - 1)\n",
    "    rpos = torch.where(q_valid, e_bms[b_idx, m_idx, qc].to(torch.long), torch.full_like(q, -1))\n",
    "    nxtp = rpos + 1\n",
    "    clen = c_len_bm[b_idx, m_idx].to(torch.long)\n",
    "    valid2 = (rpos >= 0) & (nxtp < clen)\n",
    "    nxtp_c = torch.clamp(nxtp, 0, c_bmt.size(2) - 1)\n",
    "    cval = c_bmt[b_idx, m_idx, nxtp_c].to(torch.long)\n",
    "    result = torch.where(q_valid & valid2, cval, torch.full_like(cval, -1))\n",
    "\n",
    "    if pos_mask_bt is not None:\n",
    "        pmask = pos_mask_bt.unsqueeze(1).unsqueeze(-1).expand(B, M, T, topk).reshape(-1)\n",
    "        result = torch.where(pmask, result, torch.full_like(result, -2))\n",
    "\n",
    "    return result.view(B, M, T, topk).to(torch.int16)\n",
    "\n",
    "class MultiRouteLCGFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx,\n",
    "                v_in: torch.Tensor,\n",
    "                logits_all: torch.Tensor,\n",
    "                y_idx: torch.Tensor,\n",
    "                E_stack: torch.Tensor,\n",
    "                idx_topk_bmt: torch.Tensor,\n",
    "                y_cf_bmtk: torch.Tensor):\n",
    "        ctx.save_for_backward(logits_all, y_idx, E_stack, idx_topk_bmt, y_cf_bmtk)\n",
    "        return v_in\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_v: torch.Tensor):\n",
    "        (logits_all, y_idx, E_stack, idx_topk_bmt, y_cf_bmtk) = ctx.saved_tensors\n",
    "        device = logits_all.device\n",
    "        B, T, M, K = logits_all.shape\n",
    "        topk = idx_topk_bmt.size(-1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            p_all = torch.softmax(logits_all.float(), dim=-1)\n",
    "            p_all_bmtk = p_all.permute(0,2,1,3).contiguous()\n",
    "            probs_topk = torch.gather(p_all_bmtk, dim=-1, index=idx_topk_bmt)\n",
    "\n",
    "            g = grad_v.float()\n",
    "            S = torch.einsum(\"btd,mkd->bmtk\", g, E_stack.float())\n",
    "\n",
    "            y_old_bmt = y_idx.permute(0,2,1).contiguous()\n",
    "            y_old_exp = y_old_bmt.unsqueeze(-1).expand(-1,-1,-1,topk)\n",
    "\n",
    "            y_cf = y_cf_bmtk.to(torch.long)\n",
    "            skip_mask = (y_cf == -2)\n",
    "            pad_mask  = (y_cf == -1)\n",
    "            idx_new   = torch.where(pad_mask, torch.zeros_like(y_cf), y_cf + 1)\n",
    "\n",
    "            S_new = torch.gather(S, dim=-1, index=idx_new)\n",
    "            S_old = torch.gather(S, dim=-1, index=y_old_exp)\n",
    "            deltas = torch.where(skip_mask, torch.zeros_like(S_new), S_new - S_old)\n",
    "\n",
    "            mean_delta = (probs_topk * deltas).sum(dim=-1, keepdim=True)\n",
    "            grad_topk  = probs_topk * (deltas - mean_delta)\n",
    "\n",
    "            grad_logits = torch.zeros_like(logits_all, dtype=p_all.dtype, device=device)\n",
    "            grad_logits_flat = grad_logits.permute(0,2,1,3).contiguous().view(B*M*T, K)\n",
    "            idx_flat = idx_topk_bmt.contiguous().view(B*M*T, topk)\n",
    "            src_flat = grad_topk.contiguous().view(B*M*T, topk)\n",
    "            grad_logits_flat.scatter_add_(1, idx_flat, src_flat)\n",
    "            grad_logits = grad_logits_flat.view(B,M,T,K).permute(0,2,1,3).to(logits_all.dtype)\n",
    "\n",
    "        return grad_v, grad_logits, None, None, None, None\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class FixedLenLMCollator:\n",
    "    pad_token_id: int\n",
    "    seq_len: int\n",
    "\n",
    "    def __call__(self, features):\n",
    "        input_ids = [f[\"input_ids\"][: self.seq_len] for f in features]\n",
    "        if \"attention_mask\" in features[0]:\n",
    "            attention_mask = [f[\"attention_mask\"][: self.seq_len] for f in features]\n",
    "        else:\n",
    "            attention_mask = [[1] * len(x) for x in input_ids]\n",
    "\n",
    "        input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
    "        attention_mask = torch.tensor(attention_mask, dtype=torch.long)\n",
    "        if input_ids.shape[1] < self.seq_len:\n",
    "            pad_len = self.seq_len - input_ids.shape[1]\n",
    "            pad = torch.full((input_ids.size(0), pad_len), self.pad_token_id, dtype=torch.long)\n",
    "            input_ids = torch.cat([input_ids, pad], dim=1)\n",
    "            attention_mask = torch.cat([attention_mask, torch.zeros_like(pad)], dim=1)\n",
    "\n",
    "        if \"labels\" in features[0]:\n",
    "            labels = [f[\"labels\"][: self.seq_len] for f in features]\n",
    "            labels = torch.tensor(labels, dtype=torch.long)\n",
    "            if labels.shape[1] < self.seq_len:\n",
    "                lab_pad = torch.full((labels.size(0), self.seq_len - labels.shape[1]), -100, dtype=torch.long)\n",
    "                labels = torch.cat([labels, lab_pad], dim=1)\n",
    "        else:\n",
    "            labels = input_ids.clone()\n",
    "            labels[labels == self.pad_token_id] = -100\n",
    "\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional, Tuple, List\n",
    "from transformers.models.qwen3.modeling_qwen3 import Qwen3ForCausalLM, Qwen3DecoderLayer\n",
    "\n",
    "def patch_qwen3_with_multiroute_rosa(model: Qwen3ForCausalLM):\n",
    "    import os\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    from transformers.models.qwen3.modeling_qwen3 import Qwen3DecoderLayer\n",
    "\n",
    "    base_param = model.model.embed_tokens.weight\n",
    "    base_dtype = base_param.dtype\n",
    "    base_device = base_param.device\n",
    "\n",
    "    hidden_size = model.config.hidden_size\n",
    "    M = int(ROSA_NUM_ROUTES)\n",
    "    K = int(ROSA_VOCAB_SIZE)\n",
    "\n",
    "    inject_mode = os.environ.get(\"ROSA_INJECT_MODE\", \"\").lower()\n",
    "    lcg_enable  = os.environ.get(\"LCG_ENABLE\", \"\") in (\"1\", \"true\", \"True\")\n",
    "\n",
    "    for li, layer in enumerate(model.model.layers):\n",
    "        if li == 0:\n",
    "            continue\n",
    "\n",
    "        layer.rosa_wlm_list = nn.ModuleList(\n",
    "            [nn.Linear(hidden_size, K, bias=False).to(dtype=base_dtype, device=base_device) for _ in range(M)]\n",
    "        )\n",
    "        for w in layer.rosa_wlm_list:\n",
    "            nn.init.xavier_uniform_(w.weight)\n",
    "\n",
    "        layer.rosa_emb_list = nn.ModuleList(\n",
    "            [nn.Embedding(K + 1, hidden_size).to(dtype=base_dtype, device=base_device) for _ in range(M)]\n",
    "        )\n",
    "        for emb in layer.rosa_emb_list:\n",
    "            nn.init.normal_(emb.weight, mean=0.0, std=0.02)\n",
    "            with torch.no_grad():\n",
    "                emb.weight.data[0].zero_()\n",
    "\n",
    "        layer.rosa_num_routes = M\n",
    "        layer.rosa_vocab_size = K\n",
    "\n",
    "        def _forward_with_multiroute_rosa(self: Qwen3DecoderLayer, hidden_states: torch.Tensor, \n",
    "                                        attention_mask: Optional[torch.Tensor] = None,\n",
    "                                        position_ids: Optional[torch.LongTensor] = None,\n",
    "                                        past_key_values=None, use_cache: Optional[bool] = False,\n",
    "                                        cache_position: Optional[torch.LongTensor] = None,\n",
    "                                        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n",
    "                                        **kwargs):\n",
    "            residual = hidden_states\n",
    "            B, T, H = hidden_states.shape\n",
    "            M = self.rosa_num_routes\n",
    "            K = self.rosa_vocab_size\n",
    "            device = hidden_states.device\n",
    "\n",
    "            u_head = self.input_layernorm(hidden_states)\n",
    "            w_stack = torch.stack([w.weight for w in self.rosa_wlm_list], dim=0)\n",
    "            logits_all = torch.einsum(\"bth,mkh->btmk\", u_head, w_stack.to(u_head.dtype))\n",
    "\n",
    "            z_gpu = torch.argmax(logits_all, dim=-1).to(torch.int32)\n",
    "            z_host = _PINNED_POOL.get(\"z_host\", (B, T, M), dtype=torch.int32)\n",
    "            z_host.copy_(z_gpu, non_blocking=True)\n",
    "\n",
    "            p_all_bmtk = logits_all.permute(0,2,1,3).contiguous()\n",
    "            topk = min(LCG_TOPK, K)\n",
    "            idx_topk_bmt = torch.topk(p_all_bmtk, k=topk, dim=-1).indices\n",
    "\n",
    "            cand_host = _PINNED_POOL.get(\"cand_host\", (B, M, T, topk), dtype=torch.int32)\n",
    "            cand_host.copy_(idx_topk_bmt.to(torch.int32), non_blocking=True)\n",
    "\n",
    "            pos_mask_cpu = None\n",
    "            if os.environ.get(\"LCG_ENABLE\", \"\") in (\"1\",\"true\",\"True\"):\n",
    "                if LCG_POS_SUBSAMPLE < 1.0:\n",
    "                    mask = (torch.rand((B, T), device=device) < LCG_POS_SUBSAMPLE)\n",
    "                    pos_mask_cpu = mask.detach().to(\"cpu\", non_blocking=True, dtype=torch.bool)\n",
    "\n",
    "            ev = torch.cuda.Event(); ev.record(torch.cuda.current_stream())\n",
    "            def _work():\n",
    "                _wait_event(ev)\n",
    "                return _rosa_lcg_prefix_ext(z_host, cand_host, pos_mask_cpu, int(K))\n",
    "            y_cpu, ycf_cpu = _ROSA_THREAD_POOL.submit(_work).result()\n",
    "\n",
    "            y_btm = y_cpu.to(device=device, dtype=torch.long, non_blocking=True)\n",
    "            y_idx = torch.where(y_btm >= 0, y_btm + 1, torch.zeros_like(y_btm))\n",
    "            y_cf_bmtk = ycf_cpu.to(device=device, dtype=torch.int16, non_blocking=True)\n",
    "\n",
    "            W_stack = torch.stack([emb.weight for emb in self.rosa_emb_list], dim=0)\n",
    "            W_flat  = W_stack.view(M*(K+1), H)\n",
    "            offsets = (torch.arange(M, device=device, dtype=y_idx.dtype) * (K + 1)).view(1,1,M)\n",
    "            idx_flat = (y_idx + offsets).reshape(-1)\n",
    "            e_flat = W_flat.index_select(0, idx_flat)\n",
    "            e_btmh = e_flat.view(B, T, M, H)\n",
    "            v = (e_btmh.sum(dim=2) / float(M)).to(u_head.dtype)\n",
    "            E_stack_gpu = W_stack.detach()\n",
    "\n",
    "            if os.environ.get(\"LCG_ENABLE\", \"\") in (\"1\",\"true\",\"True\"):\n",
    "                v = MultiRouteLCGFunction.apply(v, logits_all, y_idx, E_stack_gpu,\n",
    "                                                idx_topk_bmt, y_cf_bmtk)\n",
    "\n",
    "            if os.environ.get(\"ROSA_INJECT_MODE\",\"\").lower() == \"pre_attn\":\n",
    "                u_attn = self.input_layernorm(hidden_states + v)\n",
    "                attn_out, _ = self.self_attn(hidden_states=u_attn, attention_mask=attention_mask,\n",
    "                                            position_ids=position_ids, past_key_values=past_key_values,\n",
    "                                            use_cache=use_cache, cache_position=cache_position,\n",
    "                                            position_embeddings=position_embeddings, **kwargs)\n",
    "                hidden_states = residual + attn_out\n",
    "            else:\n",
    "                attn_out, _ = self.self_attn(hidden_states=u_head, attention_mask=attention_mask,\n",
    "                                            position_ids=position_ids, past_key_values=past_key_values,\n",
    "                                            use_cache=use_cache, cache_position=cache_position,\n",
    "                                            position_embeddings=position_embeddings, **kwargs)\n",
    "                hidden_states = residual + attn_out + v\n",
    "\n",
    "            residual2 = hidden_states\n",
    "            hidden_states = self.post_attention_layernorm(hidden_states)\n",
    "            hidden_states = self.mlp(hidden_states)\n",
    "            hidden_states = residual2 + hidden_states\n",
    "            return hidden_states\n",
    "\n",
    "\n",
    "        layer.forward = _forward_with_multiroute_rosa.__get__(layer, Qwen3DecoderLayer)\n",
    "\n",
    "    meta = {\n",
    "        \"apply_layers_from\": None,\n",
    "        \"num_routes_per_layer\": ROSA_NUM_ROUTES,\n",
    "        \"vocab_per_route\": ROSA_VOCAB_SIZE,\n",
    "        \"inject_mode\": inject_mode,\n",
    "    }\n",
    "    with open(os.path.join(OUTPUT_DIR, \"\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def build_model_and_tokenizer() -> Tuple[Qwen3ForCausalLM, AutoTokenizer]:\n",
    "    config = AutoConfig.from_pretrained(MODEL_LOCAL_DIR)\n",
    "    config.sliding_window = ATTN_WINDOW\n",
    "    config.max_window_layers = FIRST_GLOBAL_LAYERS\n",
    "    if (not hasattr(config, \"layer_types\")) or (config.layer_types is None):\n",
    "        config.layer_types = [\n",
    "            \"full_attention\" if i < config.max_window_layers else \"sliding_attention\"\n",
    "            for i in range(config.num_hidden_layers)\n",
    "        ]\n",
    "    if hasattr(config, \"attn_implementation\"):\n",
    "        config.attn_implementation = \"\" if USE_FLASH_ATTN else \"\"\n",
    "    else:\n",
    "        config._attn_implementation = \"\" if USE_FLASH_ATTN else \"\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_LOCAL_DIR, use_fast=True)\n",
    "    model = Qwen3ForCausalLM.from_pretrained(\n",
    "        MODEL_LOCAL_DIR,\n",
    "        config=config,\n",
    "        torch_dtype=torch.bfloat16 if BF16 else torch.float16,\n",
    "        low_cpu_mem_usage=True,\n",
    "    )\n",
    "\n",
    "    if GRADIENT_CHECKPOINTING:\n",
    "        model.gradient_checkpointing_enable()\n",
    "    else:\n",
    "        model.gradient_checkpointing_disable()\n",
    "\n",
    "    model.config.use_cache = False\n",
    "\n",
    "    patch_qwen3_with_multiroute_rosa(model)\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def save_rosa_only(model: Qwen3ForCausalLM, out_dir: str):\n",
    "    state = {}\n",
    "    for i, layer in enumerate(model.model.layers):\n",
    "        if hasattr(layer, \"rosa_wlm_list\"):\n",
    "            for m, head in enumerate(layer.rosa_wlm_list):\n",
    "                state[f\"model.layers.{i}.rosa_wlm_list.{m}.weight\"] = head.weight.detach().cpu()\n",
    "        if hasattr(layer, \"rosa_emb_list\"):\n",
    "            for m, emb in enumerate(layer.rosa_emb_list):\n",
    "                state[f\"model.layers.{i}.rosa_emb_list.{m}.weight\"] = emb.weight.detach().cpu()\n",
    "    path = os.path.join(out_dir, SAVE_STATE_DICT_NAME)\n",
    "    torch.save(state, path)\n",
    "    print(f\"[save] saved ROSA-only params to: {path}\")\n",
    "\n",
    "\n",
    "def build_optimizer_params(model):\n",
    "    wlm_params, emb_params, backbone_params = [], [], []\n",
    "    for n, p in model.named_parameters():\n",
    "        if \"rosa_wlm_list\" in n:\n",
    "            wlm_params.append(p)\n",
    "        elif \"rosa_emb_list\" in n:\n",
    "            emb_params.append(p)\n",
    "        else:\n",
    "            backbone_params.append(p)\n",
    "\n",
    "    param_groups = [\n",
    "        {\"params\": wlm_params, \"lr\": LR_ROSA, \"weight_decay\": WEIGHT_DECAY},\n",
    "        {\"params\": emb_params, \"lr\": LR_ROSA, \"weight_decay\": 0.0},\n",
    "    ]\n",
    "    if LR_BACKBONE and LR_BACKBONE > 0.0:\n",
    "        no_decay, has_decay = [], []\n",
    "        for n, p in model.named_parameters():\n",
    "            if (\"rosa_\" in n):\n",
    "                continue\n",
    "            if any(k in n.lower() for k in [\"bias\", \"norm\", \"layernorm\", \"ln\"]):\n",
    "                no_decay.append(p)\n",
    "            else:\n",
    "                has_decay.append(p)\n",
    "        param_groups += [\n",
    "            {\"params\": has_decay, \"lr\": LR_BACKBONE, \"weight_decay\": WEIGHT_DECAY},\n",
    "            {\"params\": no_decay, \"lr\": LR_BACKBONE, \"weight_decay\": 0.0},\n",
    "        ]\n",
    "    else:\n",
    "        for p in backbone_params:\n",
    "            p.requires_grad_(False)\n",
    "\n",
    "    return param_groups\n",
    "\n",
    "\n",
    "from transformers.trainer_callback import TrainerCallback\n",
    "\n",
    "def is_main_process() -> bool:\n",
    "    return _env_int(\"RANK\", 0) == 0\n",
    "\n",
    "class RosaZeroRowCallback(TrainerCallback):\n",
    "    def on_init_end(self, args, state, control, model=None, **kwargs):\n",
    "        if model is not None:\n",
    "            with torch.no_grad():\n",
    "                for layer in model.model.layers:\n",
    "                    if hasattr(layer, \"rosa_emb_list\"):\n",
    "                        for emb in layer.rosa_emb_list:\n",
    "                            emb.weight.data[0].zero_()\n",
    "        return control\n",
    "\n",
    "    def on_step_end(self, args, state, control, model=None, **kwargs):\n",
    "        if model is not None:\n",
    "            with torch.no_grad():\n",
    "                for layer in model.model.layers:\n",
    "                    if hasattr(layer, \"rosa_emb_list\"):\n",
    "                        for emb in layer.rosa_emb_list:\n",
    "                            emb.weight.data[0].zero_()\n",
    "        return control\n",
    "\n",
    "\n",
    "def main():\n",
    "    set_seed(SEED)\n",
    "\n",
    "    raw = load_from_disk(DATASET_DIR)\n",
    "    train_ds = raw[\"train\"]\n",
    "    test_ds = raw.get(\"test\", raw[\"validation\"] if \"validation\" in raw else None)\n",
    "    assert test_ds is not None, \"requires test or validation split\"\n",
    "\n",
    "    model, tokenizer = build_model_and_tokenizer()\n",
    "    pad_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
    "    data_collator = FixedLenLMCollator(pad_token_id=pad_id, seq_len=SEQ_LEN)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        per_device_train_batch_size=PER_DEVICE_TRAIN_BSZ,\n",
    "        per_device_eval_batch_size=None,\n",
    "        gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
    "        learning_rate=LR_ROSA,\n",
    "        warmup_steps=WARMUP_STEPS,\n",
    "        logging_steps=LOGGING_STEPS,\n",
    "        eval_steps=EVAL_STEPS,\n",
    "        save_strategy=\"\",\n",
    "        report_to=\"\",\n",
    "        fp16=(not BF16) and torch.cuda.is_available(),\n",
    "        bf16=BF16,\n",
    "        dataloader_num_workers=None,\n",
    "        gradient_checkpointing=GRADIENT_CHECKPOINTING,\n",
    "        remove_unused_columns=False,\n",
    "        optim=\"\",\n",
    "    )\n",
    "\n",
    "    optimizer_params = build_optimizer_params(model)\n",
    "\n",
    "    class _Trainer(Trainer):\n",
    "        def create_optimizer(self):\n",
    "            if self.optimizer is None:\n",
    "                self.optimizer = torch.optim.AdamW(optimizer_params, betas=(0.9, 0.98), eps=1e-8)\n",
    "            return self.optimizer\n",
    "\n",
    "    trainer = _Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=test_ds,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        callbacks=[RosaZeroRowCallback()],\n",
    "    )\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    print(f\"total_params: {total_params:,}\")\n",
    "\n",
    "    trainer.train()\n",
    "    metrics = trainer.evaluate()\n",
    "\n",
    "    meta = {\n",
    "        \"model_local_dir\": MODEL_LOCAL_DIR,\n",
    "        \"dataset_dir\": DATASET_DIR,\n",
    "        \"seq_len\": SEQ_LEN,\n",
    "        \"attn_window\": ATTN_WINDOW,\n",
    "        \"first_global_layers\": FIRST_GLOBAL_LAYERS,\n",
    "        \"rosa\": {\n",
    "            \"num_routes\": ROSA_NUM_ROUTES,\n",
    "            \"vocab_size\": ROSA_VOCAB_SIZE,\n",
    "            \"lcg_topk\": LCG_TOPK,\n",
    "            \"pos_subsample\": LCG_POS_SUBSAMPLE,\n",
    "            \"lr_rosa\": LR_ROSA, \"lr_backbone\": LR_BACKBONE,\n",
    "            \"k_per_layer\": {str(i): (getattr(model.model.layers[i], \"rosa_vocab_size\", 0))\n",
    "                            for i in range(model.config.num_hidden_layers)}\n",
    "        },\n",
    "        \"metrics\": metrics,\n",
    "        \"time\": time.asctime(),\n",
    "    }\n",
    "\n",
    "    if is_main_process():\n",
    "        print(\"[eval] metrics:\", metrics)\n",
    "        save_rosa_only(model, OUTPUT_DIR)\n",
    "        with open(os.path.join(OUTPUT_DIR, \"\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"[done] meta saved at {os.path.join(OUTPUT_DIR, '')}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
